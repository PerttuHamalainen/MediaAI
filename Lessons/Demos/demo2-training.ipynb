{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04af485a",
   "metadata": {},
   "source": [
    "# Demo 2: Training a Neural Network\n",
    "\n",
    "* Using PyTorch\n",
    "* Expected time: 5 minutes\n",
    "\n",
    "\n",
    "Let's recall together what we need to train a NN:\n",
    "* NN\n",
    "* Input/output representation\n",
    "* Data\n",
    "* Differentiable programming\n",
    "* Loss function\n",
    "* Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7407771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "torch.manual_seed(0)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4220d0",
   "metadata": {},
   "source": [
    "## Step 1: Data\n",
    "\n",
    "For this demo, we will use the MNIST digit classification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cccd5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = MNIST(\"mnist\", download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82971a0",
   "metadata": {},
   "source": [
    "It's always a good idea to visualize and analyze the data to gain some understanding of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7bc9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many images are there in the dataset?\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19621b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the format of the data?\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddbf1dd",
   "metadata": {},
   "source": [
    "It's a tuple of two elements. In Python, it's easy enough to unpack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a643e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99c3850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What does each \"example\" look like?\n",
    "print(np.array(input))\n",
    "print(np.array(input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e9a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(f\"label: {label}\")\n",
    "plt.imshow(np.array(input), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9b3e0f",
   "metadata": {},
   "source": [
    "Aha, the handwriting of the digit \"5\" corresponds to the label \"5\", so we have 60000 grayscale images whose sizes are 28x28 and the corresponding numerical labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa284f",
   "metadata": {},
   "source": [
    "## Step 2: Input/Output Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803ceabe",
   "metadata": {},
   "source": [
    "For our model to process the image, it's more convenient to \"flatten\" the images to be a long vector instead of a 28x28 matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a890e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_reshaped = np.array(input).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_reshaped)\n",
    "print(input_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27487668",
   "metadata": {},
   "source": [
    "So instead of 28x28 images, we are working with 784-dimensional vectors. (28x28 = 784)\n",
    "\n",
    "As for the output representation, remember that we had 1000-dimensional output for our ImageNet demo earlier. Now, we have the digit classification problem, meaning...\n",
    "\n",
    "Q: What would our predictions look like, when the neural network produces it?\n",
    "* A. \"5\" or something like that\n",
    "* B. An image of the digit 5\n",
    "* C. A 10-dimensional array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa18123",
   "metadata": {},
   "source": [
    "## Step 3: Differentiable Programming\n",
    "\n",
    "Libraries like PyTorch, TensorFlow, Caffe, MXNet and more allow us to simplify training neural networks with more interpretable code. Differentiable programming libraries usually work with \"tensors\" which are basically the same as arrays, but they have a special property where differentiable operations are recorded into the variables. Easier to show this than tell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4f7fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(input_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315ef2a3",
   "metadata": {},
   "source": [
    "Q: What do you think input_tensor looks like?\n",
    "* A. Basically same as input_reshaped\n",
    "* B. Different from input_reshaped\n",
    "* C. None of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_tensor)\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f529d8",
   "metadata": {},
   "source": [
    "Now, we will instantiate our neural network. In PyTorch's language, producing predictions from a neural network (model) is a differentiable operation, which we will see soon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11787abe",
   "metadata": {},
   "source": [
    "## Step 4: Neural Network\n",
    "\n",
    "In this step, we will instantiate a neural network using PyTorch's `nn.Sequential` class. `nn.Sequential` lets us express a sequence of differentiable operations quite gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83450cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7e5a9",
   "metadata": {},
   "source": [
    "## Pop Quiz!\n",
    "\n",
    "Q: What do you think `nn.Linear(784, 16)` is about?\n",
    "\n",
    "* A. a matrix multiplication that reduces a 784-dimensional vector to a 16-dimensional vector\n",
    "* B. a linear interpolation between a 784-dimensional vector and a 16-dimensional vector\n",
    "* C. none of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3f212",
   "metadata": {},
   "source": [
    "Let's see what the output looks like when we put our 784-dimensional tensor through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0665a3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# forward() is basically the same as predict(), \n",
    "# but it's says more explicitly that the operation is differentiable.\n",
    "# the term \"forward\" comes from \"feedforward\" in backpropagation.\n",
    "pred_tensor = model.forward(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d96209",
   "metadata": {},
   "source": [
    "Oops, looks like the data types are incompatible. Here come some dull/cryptic bits when working with differentiable programming libraries: you must make sure that the data types and input shapes are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8239551d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_tensor = model.forward(input_tensor.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf7e4d",
   "metadata": {},
   "source": [
    "Whew! So we needed to massage our input image as follows:\n",
    "* Turn the image into a 28x28 array\n",
    "* Turn the 28x28 array in to a 784-dimensional array\n",
    "* Turn the array in to a tensor\n",
    "* Cast the data type of the tensor to float to be consistent with expected data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fb5d0",
   "metadata": {},
   "source": [
    "Let's look at the shape of `pred_tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb5ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b584850",
   "metadata": {},
   "source": [
    "Q: Is there any difference from what you were expecting?\n",
    "\n",
    "Now, let's look at what's inside `pred_tensor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562d1a06",
   "metadata": {},
   "source": [
    "Q: Do you think `pred_tensor` would be a \"correct\" prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1aae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pred_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b73c668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.argmax(pred_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52674342",
   "metadata": {},
   "source": [
    "Of course not! Since the network is completely untrained and it has no idea what a digit 5 looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec801a53",
   "metadata": {},
   "source": [
    "## Step 5: Loss Function\n",
    "\n",
    "As you see, initializing a NN and just producing a prediction doesn't do anything. We must provide a \"training signal\" to the network using a loss function, as discussed in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eeeddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_tensor = torch.tensor(label)\n",
    "print(torch.argmax(pred_tensor), label_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdee3d0",
   "metadata": {},
   "source": [
    "So right now the neural network thinks the image is a \"3\". Of course, it has no idea what it's saying. Let's give it a feedback of a sort, expressed via a *loss function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cacb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We won't go into details about what CrossEntropyLoss is, but just know that this is the correct loss\n",
    "# for our task and current format of input/output representations\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another shape massaging, but I won't get into it too much. Once you wrangle tensors for long enough, \n",
    "# it becomes pretty clear.\n",
    "loss = cross_entropy_loss.forward(pred_tensor.unsqueeze(0), label_tensor.unsqueeze(0))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18164c15",
   "metadata": {},
   "source": [
    "Aha, now we successfully quantified the performance of our current NN.\n",
    "\n",
    "Q: Making an NN \"better\" would correspond to...\n",
    "* A. Minimizing the value coming from `cross_entropy_loss.forward()`\n",
    "* B. Maximizing the value coming from `cross_entropy_loss.forward()`\n",
    "* C. None of the above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8de6a3",
   "metadata": {},
   "source": [
    "## Step 6: Optimization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cfd231",
   "metadata": {},
   "source": [
    "The very last step for us is to use an optimization algorithm to optimize the loss value. How? Well, it's a pretty complex concept. For now, I'll present you PyTorch's beautifully elegant way to use optimization algorithms out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b38067",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=3e-4, params=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be33a71d",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "* \"Adam\" is a widely popular optimization algorithm that has 10k+ citations. It's not perfect, but it's a great starting point.\n",
    "* `lr` stands for learning rate. For now, I'm using some number according to my intuitive guess of what a good learning rate looks like.\n",
    "* `params` specifies what I'm changing in order to make the loss function lower, hence the connections of our NN.\n",
    "\n",
    "Optimizers are easy to use, if you forgive some arbitrary-looking steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64f890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519164bb",
   "metadata": {},
   "source": [
    "Just for the demonstration of what that chunk of code just did, I'll redo the prediction and loss calculation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tensor = model.forward(input_tensor.float().reshape(1, -1))\n",
    "loss = cross_entropy_loss.forward(pred_tensor, label_tensor.reshape(-1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e766dd",
   "metadata": {},
   "source": [
    "Q: Note the differences in number. What do you think happened?\n",
    "* A. The parameters of our NN got changed by optimizer\n",
    "* B. The prediction from our NN got changed\n",
    "* C. All of the above\n",
    "\n",
    "If I did this many many times, my NN would know what the correct label is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f437ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(1000)):\n",
    "    # get the latest version of prediction\n",
    "    pred_tensor = model.forward(input_tensor.float().reshape(1, -1))\n",
    "    # compute loss based on predictions (note that label_tensor never changes!)\n",
    "    loss = cross_entropy_loss.forward(pred_tensor, label_tensor.reshape(-1))\n",
    "    # perform an optimization step, which mutates our NN\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # rinse and repeat 1000 times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ba1e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1176c3d9",
   "metadata": {},
   "source": [
    "Wow, that's a pretty \"small\" number. Here's what our latest prediction looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4b8dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_tensor = model.forward(input_tensor.float().reshape(1, -1))\n",
    "print(pred_tensor)\n",
    "print(torch.argmax(pred_tensor), label_tensor.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd6acd",
   "metadata": {},
   "source": [
    "That's a huge improvement from thinking it's a \"3\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4df5f3",
   "metadata": {},
   "source": [
    "## Putting it altogether\n",
    "\n",
    "Thus far, for the sake of demonstration, we only used one example (image/label pair) to train our neural network. Of course, this is not super useful. For example, it never had the chance to see what other digits look like!\n",
    "\n",
    "Ideally, we should use all of the 60000 digits to train a decent digit classifier. For the interest of time, though, we use only 100 digits for now. Forgive a few magic lines that I don't have time to get into, but believe that all of the 6 steps discussed above are still present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb388f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(examples):\n",
    "    inputs = torch.stack([torch.tensor(np.array(input)).reshape(-1) for input, _ in examples], dim=0)\n",
    "    labels = torch.tensor([label for _, label in examples])\n",
    "    return inputs, labels    \n",
    "\n",
    "subset = Subset(dataset, np.arange(100))\n",
    "data_loader = DataLoader(subset, batch_size=100, collate_fn=collate)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 256),\n",
    ")\n",
    "optimizer = torch.optim.Adam(lr=3e-4, params=model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d5dbf",
   "metadata": {},
   "source": [
    "Q: Look at the way we instantiated `model` just now. Why would I want to use 256 as the layer size now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4302f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(100)):\n",
    "    epoch_loss = 0.\n",
    "    for example in data_loader:\n",
    "        input, label = example\n",
    "        input_tensor = input.float()\n",
    "        label_tensor = label.long()\n",
    "        \n",
    "        # gpu acceleration\n",
    "        model = model.cuda()\n",
    "        input_tensor = input_tensor.cuda()\n",
    "        label_tensor = label_tensor.cuda()\n",
    "        \n",
    "        # get the latest version of prediction\n",
    "        pred_tensor = model.forward(input_tensor / 255.)\n",
    "\n",
    "        # compute loss based on predictions (note that label_tensor never changes!)\n",
    "        loss = cross_entropy_loss.forward(pred_tensor, label_tensor)\n",
    "        \n",
    "        # perform an optimization step, which mutates our NN\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.data\n",
    "    epoch_loss /= len(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c9ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for our case, inference doesn't need gpu acceleration\n",
    "model = model.cpu()\n",
    "input_tensor = input_tensor.cpu()\n",
    "label_tensor = label_tensor.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    input, label = subset[i]\n",
    "    input_array = np.array(input)\n",
    "    input_reshaped = np.array(input).reshape(1, -1)\n",
    "    input_tensor = torch.tensor(input_reshaped).float() / 255.\n",
    "    pred_tensor = model.forward(input_tensor)\n",
    "    prob_tensor = torch.softmax(pred_tensor, dim=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(input_array, cmap=\"gray\")\n",
    "    plt.title(f\"prediction:{torch.argmax(prob_tensor).item()}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd69d2",
   "metadata": {},
   "source": [
    "# But Before You Go...\n",
    "\n",
    "Is our NN actually useful in the wild? i.e. if I show more digit images that were NOT used during training, what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(100, 111):\n",
    "    input, label = dataset[i]\n",
    "    input_array = np.array(input)\n",
    "    input_reshaped = np.array(input).reshape(1, -1)\n",
    "    input_tensor = torch.tensor(input_reshaped).float() / 255.\n",
    "    pred_tensor = model.forward(input_tensor)\n",
    "    prob_tensor = torch.softmax(pred_tensor, dim=1)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(input_array, cmap=\"gray\")\n",
    "    plt.title(f\"prediction:{torch.argmax(prob_tensor).item()}\")\n",
    "#     print(torch.argmax(pred_tensor), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcab94f",
   "metadata": {},
   "source": [
    "Lots of errors start showing up. This is called *overfitting*. We won't be getting too much in detail about this, but be aware that for your NN to be useful at inference time, you need more \"general\" performance, usually achieved by e.g.:\n",
    "* Using more data\n",
    "* Using sparser architectures\n",
    "* Using augmentations\n",
    "* Using better *hyperparameters*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
