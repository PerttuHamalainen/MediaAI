{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s3mPrBIHhts"
      },
      "source": [
        "## Tutorial and exercises: Few-Shot prompting for non-programmers\n",
        "*This Colab notebook is part of the [AI for media, art, and design](https://github.com/PerttuHamalainen/MediaAI) course of Aalto University and also used on the [Art of Writing](https://aalto.my.site.com/courses/s/course/a057T0000043wJBQAY/art-of-writing?language=en_US) course.*\n",
        "\n",
        "This notebook demonstrates and lets you practice few-shot prompting of Large Language Models (LLMs).\n",
        "\n",
        "**What is few-shot prompting?**\n",
        "\n",
        "Few-shot prompting means that one includes *high-quality examples* of desired outputs in the LLM prompt. This can greatly improve the generated results. Similar to explaining things to a human, using concrete examples is often the most *efficient and precise way* to explain what one wants.\n",
        "\n",
        "*Finding/writing the few-shot examples is a key skill to practise* in AI-assisted creative writing. Typically, this includes both writing example text yourself, and browsing and/or scraping the Internet for examples.\n",
        "\n",
        "**Learning goals:**\n",
        "\n",
        "* Practice producing the few-shot examples and learn to anticipate how different examples change the results\n",
        "* Compare different LLMs and understand the differences between base models and finetuned models, as well as continuation models and chat models.\n",
        "* Practice combinatorial few-shot prompting. For example, we will create game ideas that systematically explore combinations of game genres and mechanics\n",
        "\n",
        "**New to Colab notebooks?**\n",
        "\n",
        "Colab notebooks are browser-based learning environments consisting of *cells* that include either text or code. The code is executed in a Google virtual machine instead of your own computer. You can run code cell-by-cell (click the \"play\" symbol of each code cell), and selecting \"Run all\" as instructed above is usually the first step to verify that everything works. For more info, see Google's [Intro video](https://www.youtube.com/watch?v=inN8seMm7UI) and [curated example notebooks](https://colab.google/notebooks/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting started\n",
        "1. Select \"Run all\" from the Runtime menu. This will run all the code in this notebook so that you can inspect the results.\n",
        "2. Below, enter your OpenAI API key when requested. The class instructor should give you a temporary key. If you want to use your own key, follow [OpenAI's instructions](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)\n",
        "3. Proceed through the notebook, following the exercise instructions.\n",
        "\n",
        "Some exercises will ask you to edit the code, but this is limited to only defining the prompt and requires no programming knowledge. The code you should not touch is clearly marked."
      ],
      "metadata": {
        "id": "fhAe-DhATr3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from getpass import getpass\n",
        "try: OPENAI_API_KEY\n",
        "except NameError: OPENAI_API_KEY = getpass('Enter OpenAI API key: ')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "abU_u9RXvu44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do some setup and connect to OpenAI.\n",
        "This part will install and import libraries. It will take a while, but only needs to be done once.\n",
        "\n",
        "A rotating \"stop\" symbol below means the code is executing and you should wait."
      ],
      "metadata": {
        "id": "OvXpXUB5fEwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX7TEywGVUxX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#First, install and import code packages\n",
        "#! in the beginning of a Colab code line allows running Linux shell commands\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install umap-learn\n",
        "import tiktoken\n",
        "import time\n",
        "import asyncio\n",
        "import pickle\n",
        "import hashlib\n",
        "import os\n",
        "import openai\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import plotly.express as px\n",
        "import textwrap\n",
        "\n",
        "client=openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "client_async=openai.AsyncOpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To make things easier, this cell defines some code helpers.\n",
        "# source: https://github.com/PerttuHamalainen/LLMCode\n",
        "import json\n",
        "\n",
        "#Colab is already running an asyncio event loop => need this hack for async OpenAI API calling\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#progress bar helper\n",
        "def print_progress_bar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
        "    \"\"\"\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "        print()\n",
        "\n",
        "#LLM response cache. For reducing API costs, if the exact same prompt has been\n",
        "#used before, the helpers below offer an option to return results from cache.\n",
        "cache_dir = \"./llm_cache\"\n",
        "def cache_keys_equal(key1,key2):\n",
        "    if (type(key1) is np.ndarray) and (type(key2) is np.ndarray):\n",
        "        return np.array_equal(key1,key2)\n",
        "    return key1==key2\n",
        "\n",
        "def cache_hash(key):\n",
        "    return hashlib.md5(key).hexdigest()\n",
        "\n",
        "def load_cached(key):\n",
        "    cached_name= cache_dir + \"/\" + cache_hash(key)\n",
        "    if os.path.exists(cached_name):\n",
        "        cached=pickle.load(open(cached_name,\"rb\"))\n",
        "        if cache_keys_equal(cached[\"key\"],key):\n",
        "            #cache_copy_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"cache_copy\") #for debugging which files are actually used...\n",
        "            #shutil.copy(cached_name, cache_copy_dir+\"/\" + cache_hash(key))\n",
        "            return cached[\"value\"]\n",
        "    return None\n",
        "\n",
        "def cache(key,value):\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.mkdir(cache_dir)\n",
        "    cached_name= cache_dir + \"/\" + cache_hash(key)\n",
        "    pickle.dump({\"key\":key,\"value\":value},open(cached_name,\"wb\"))\n",
        "\n",
        "\n",
        "#Some info for tokenizing text (e.g., for calculating the number of prompt tokens)\n",
        "tiktoken_encodings = {\n",
        "    \"gpt-4-turbo\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-4-turbo-preview\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-4\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-3.5-turbo\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-3.5-turbo-instruct\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-3.5-turbo-16k\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"davinci-002\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"text-davinci-003\": tiktoken.get_encoding(\"p50k_base\"),\n",
        "    \"text-davinci-002\": tiktoken.get_encoding(\"p50k_base\"),\n",
        "    \"text-davinci-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"text-curie-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"text-babbage-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"text-ada-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"davinci\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"curie\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"babbage\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"ada\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "}\n",
        "\n",
        "#Maximum number of tokens supported by different models\n",
        "max_llm_context_length = {\n",
        "    \"gpt-4-turbo\": 16384*2,\n",
        "    \"gpt-4-turbo-preview\": 16384*2,\n",
        "    \"gpt-3.5-turbo-16k\": 16384,\n",
        "    \"gpt-4\": 8192,\n",
        "    \"gpt-3.5-turbo\": 4096,\n",
        "    \"gpt-3.5-turbo-instruct\": 4096,\n",
        "    \"text-davinci-003\": 4096,\n",
        "    \"text-davinci-002\": 4096,\n",
        "    \"text-davinci-001\": 2049,\n",
        "    \"text-curie-001\": 2049,\n",
        "    \"text-babbage-001\": 2049,\n",
        "    \"text-ada-001\": 2049,\n",
        "    \"davinci\": 2049,\n",
        "    \"curie\": 2049,\n",
        "    \"babbage\": 2049,\n",
        "    \"ada\": 2049\n",
        "}\n",
        "\n",
        "#Does a model only support the newer chat API and not the continuations API?\n",
        "def is_chat_model(model):\n",
        "    return (\"gpt-4\" in model) or (\"gpt-3.5-turbo\" in model) and (\"gpt-3.5-turbo-instruct\" not in model)\n",
        "\n",
        "#Calculate the number of tokens for a string\n",
        "def num_tokens_from_string(string: str, model: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    if not model in tiktoken_encodings:\n",
        "        raise Exception(f\"Tiktoken encoding unknown for LLM: {model}\")\n",
        "    encoding = tiktoken_encodings[model]\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Queries an LLM for continuations of a batch of prompts given as a list\n",
        "def query_LLM_batch(model, prompt_batch, max_tokens, use_cache=None, temperature=None,system_message=None,stop=None):\n",
        "    if use_cache is None:\n",
        "        use_cache=False\n",
        "    cache_key=model.join(prompt_batch).encode('utf-8')\n",
        "    if use_cache:\n",
        "        cached_result=load_cached(cache_key)\n",
        "        if cached_result is not None:\n",
        "            return cached_result\n",
        "\n",
        "    #choose whether to use the chat API or the older query API\n",
        "    if is_chat_model(model):\n",
        "        if system_message is None:\n",
        "            system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "        # each batch in the prompt becomes its own asynchronous chat completion request\n",
        "        async def batch_request(prompt_batch):\n",
        "            tasks=[]\n",
        "            for prompt in prompt_batch:\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ]\n",
        "                tasks.append(client_async.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=messages,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                    n=1,  # one completion per prompt\n",
        "                    stop=stop,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                ))\n",
        "            return await asyncio.gather(*tasks)\n",
        "\n",
        "        loop = asyncio.get_event_loop()\n",
        "        responses = loop.run_until_complete(batch_request(prompt_batch))\n",
        "        continuations = [response.choices[0].message.content.strip() for response in responses]\n",
        "\n",
        "        # before we return the continuations, ensure that we don't violate OpenAI's rate limits\n",
        "        total_tokens = 0\n",
        "        for prompt in prompt_batch:\n",
        "            total_tokens += num_tokens_from_string(string=system_message, model=model)\n",
        "            total_tokens += num_tokens_from_string(string=prompt, model=model)\n",
        "        for continuation in continuations:\n",
        "            total_tokens += num_tokens_from_string(string=continuation, model=model)\n",
        "        max_tokens_per_minute = 90000  # currently imposed limit for ChatGPT models\n",
        "        wait_seconds = (total_tokens / max_tokens_per_minute) * 60.0\n",
        "        #print(f\"Waiting {wait_seconds} seconds to ensure staying within rate limit\")\n",
        "        time.sleep(wait_seconds)\n",
        "\n",
        "    else:\n",
        "        # The old completions API supports batched prompts out-of-the-box\n",
        "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        response = client.completions.create(\n",
        "            model=model,\n",
        "            prompt=prompt_batch,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=1.0,\n",
        "            frequency_penalty=0.0,\n",
        "            presence_penalty=0.0,\n",
        "            stop=stop,\n",
        "            n=1  # one completion per prompt\n",
        "        )\n",
        "        # extract continuations\n",
        "        continuations = [choice.text for choice in response.choices]\n",
        "\n",
        "        # before we return the continuations, ensure that we don't violate OpenAI's rate limits\n",
        "        total_tokens = 0\n",
        "        for prompt in prompt_batch:\n",
        "            total_tokens += num_tokens_from_string(string=prompt, model=model)\n",
        "        for continuation in continuations:\n",
        "            total_tokens += num_tokens_from_string(string=continuation, model=model)\n",
        "        max_tokens_per_minute = 90000  # currently imposed limit for ChatGPT models\n",
        "        wait_seconds = (total_tokens / max_tokens_per_minute) * 60.0\n",
        "        #print(f\"Waiting {wait_seconds} seconds to ensure staying within rate limit\")\n",
        "        time.sleep(wait_seconds)\n",
        "\n",
        "    if use_cache:\n",
        "        cache(key=cache_key,value=continuations)\n",
        "    return continuations\n",
        "\n",
        "\n",
        "#Query LLM with a list of prompts\n",
        "def query_LLM(model, prompts, max_tokens, use_cache=None, temperature=None, system_message=None,stop=None):\n",
        "    #Query the LLM in batches\n",
        "    continuations=[]\n",
        "    batch_size = 10  # The exact max batch_size for each model is unknown. This seems to work for all, and provides a nice speed-up.\n",
        "    N = len(prompts)\n",
        "    for i in range(0, N, batch_size):\n",
        "        prompt_batch=prompts[i:min([N, i + batch_size])]\n",
        "        continuations+=query_LLM_batch(model=model,\n",
        "                                 prompt_batch=prompt_batch,\n",
        "                                 max_tokens=max_tokens,\n",
        "                                 use_cache=use_cache,\n",
        "                                 temperature=temperature,\n",
        "                                 system_message=system_message,\n",
        "                                 stop=stop)\n",
        "        print_progress_bar(min([N, i + batch_size]), N,printEnd=\"\")\n",
        "    return continuations\n",
        "\n",
        "\n",
        "\n",
        "def embed(texts,use_cache=None,model=None):\n",
        "    if model is None:\n",
        "        model = \"text-embedding-3-small\"\n",
        "    if use_cache is None:\n",
        "        use_cache = True\n",
        "    cache_key=(model+(\"\".join(texts))).encode('utf-8')\n",
        "    if use_cache:\n",
        "        cached_result=load_cached(cache_key)\n",
        "        if cached_result is not None:\n",
        "            print(\"Loaded embeddings from cache, hash\", cache_hash(cache_key))\n",
        "            return cached_result\n",
        "\n",
        "\n",
        "    #query embeddings from the API\n",
        "    texts=[json.dumps(s) for s in texts]  #make sure we escape quotes in a way compatible with GPT-3 API's internal use of json\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    batch_size = 32\n",
        "    N = len(texts)\n",
        "\n",
        "    embed_matrix=[]\n",
        "    for i in range(0, N, batch_size):\n",
        "        print_progress_bar(i, N)\n",
        "        embed_batch=texts[i:min([N, i + batch_size])]\n",
        "        embeddings = client.embeddings.create(input=embed_batch, model=model)\n",
        "        print(embeddings)\n",
        "        for j in range(len(embed_batch)):\n",
        "            embed_matrix.append(embeddings.data[j].embedding)\n",
        "    print(\"\")\n",
        "    embed_matrix=np.array(embed_matrix)\n",
        "    #dim = len(embeddings['data'][0]['embedding'])\n",
        "    #embed_matrix = np.zeros([N, dim])\n",
        "    #for i in range(N):\n",
        "    #    embed_matrix[i, :] = embeddings['data'][i]['embedding']\n",
        "\n",
        "    #update cache\n",
        "    if use_cache:\n",
        "        cache(cache_key,embed_matrix)\n",
        "\n",
        "    #return results\n",
        "    return embed_matrix\n",
        "\n",
        "\n",
        "def reduce_embedding_dimensionality(embeddings,num_dimensions,method=\"UMAP\",use_cache=True,n_neighbors=None):\n",
        "    if isinstance(embeddings,list):\n",
        "        #embeddings is a list of embedding matrices => pack all to one big matrix for joint dimensionality reduction\n",
        "        all_emb = np.concatenate(embeddings, axis=0)\n",
        "    else:\n",
        "        all_emb = embeddings\n",
        "    def unpack(x,embeddings_list):\n",
        "        row = 0\n",
        "        result = []\n",
        "        for e in embeddings_list:\n",
        "            N = e.shape[0]\n",
        "            result.append(x[row:row + N])\n",
        "            row += N\n",
        "        return result\n",
        "\n",
        "    cache_key=(str(all_emb.tobytes())+str(num_dimensions)+method+str(n_neighbors)).encode('utf-8')\n",
        "    if use_cache:\n",
        "        cached_result=load_cached(cache_key)\n",
        "        if cached_result is not None:\n",
        "            print(\"Loaded dimensionality reduction results from cache, hash \", cache_hash(cache_key))\n",
        "            if isinstance(embeddings, list):\n",
        "                return unpack(cached_result,embeddings)\n",
        "            else:\n",
        "                return cached_result\n",
        "    from sklearn.manifold import MDS\n",
        "    from sklearn.manifold import TSNE\n",
        "    import umap\n",
        "    from sklearn.decomposition import PCA\n",
        "    #cosine distance\n",
        "    all_emb=all_emb/np.linalg.norm(all_emb,axis=1,keepdims=True)\n",
        "\n",
        "    if method==\"MDS\":\n",
        "        mds=MDS(n_components=num_dimensions,dissimilarity=\"precomputed\")\n",
        "        cosine_sim = np.inner(all_emb, all_emb)\n",
        "        cosine_dist = 1 - cosine_sim\n",
        "        x=mds.fit_transform(cosine_dist)\n",
        "    elif method==\"TSNE\":\n",
        "        tsne=TSNE(n_components=num_dimensions)\n",
        "        x=tsne.fit_transform(all_emb)\n",
        "    elif method==\"PCA\":\n",
        "        pca=PCA(n_components=num_dimensions)\n",
        "        x=pca.fit_transform(all_emb)\n",
        "    elif method==\"UMAP\":\n",
        "        if n_neighbors is None:\n",
        "            n_neighbors=5\n",
        "        reducer = umap.UMAP(n_components=num_dimensions,metric='cosine',n_neighbors=n_neighbors)\n",
        "        x=reducer.fit_transform(all_emb)\n",
        "    else:\n",
        "        raise Exception(\"Invalid dimensionality reduction method!\")\n",
        "\n",
        "    if use_cache:\n",
        "        cache(cache_key,x)\n",
        "\n",
        "    if isinstance(embeddings, list):\n",
        "        return unpack(x,embeddings)\n",
        "    return x\n",
        "\n",
        "\n",
        "#some quick test methods\n",
        "def test_embeddings():\n",
        "    texts=[\"queen\",\"king\",\"man\",\"woman\"]\n",
        "    embeddings=embed(texts)\n",
        "    embeddings=reduce_embedding_dimensionality(embeddings,method=\"PCA\",num_dimensions=2)\n",
        "    df=pd.DataFrame()\n",
        "    df[\"texts\"]=texts\n",
        "    df[\"x\"]=embeddings[:,0]\n",
        "    df[\"y\"]=embeddings[:,1]\n",
        "    px.scatter(df,\n",
        "                #width=1300, height=1000, #The codes should be approximately 1:1 aspect ratio, but need space for the color bar\n",
        "                x=\"x\",\n",
        "                y=\"y\",\n",
        "                hover_name=\"texts\")\n",
        "\n",
        "def test_batched_prompting():\n",
        "    prompts=[\"what is 1+1?\",\"what is 1+2?\",\"what is 1+3?\"]\n",
        "    print(query_LLM(\"davinci-002\",prompts,max_tokens=20))\n"
      ],
      "metadata": {
        "id": "AJohDVequMZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iCU0aao2tkpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exercise: Generating game ideas and comparing different LLMs\n",
        "\n",
        "1. Run the code below 5 times to get alternative generations. Increase the \"Maximum length\" parameter if needed.\n",
        "2. Select \"davinci-002\" from the \"Model\" menu and click \"Regenerate\" 5 times. Davinci-002 is the non-finetuned base model, whereas gpt-3.5-turbo-instruct is the finetuned version.\n",
        "3. Run the code again 5 times.\n",
        "4. Reduce the temperature to 0.3\n",
        "5. Run the code again 5 times.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ng0Ik1qnrF85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "#Parameters that you can edit\n",
        "model=\"gpt-3.5-turbo-instruct\" # @param [\"davinci-002\", \"gpt-3.5-turbo-instruct\"] {allow-input: false}\n",
        "temperature=1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "max_tokens=200 # @param {type:\"slider\", min:10, max:1000, step:10}\n",
        "\n",
        "prompt=\"\"\"A list of novel and experimental indie game ideas:\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "###################################################################\n",
        "#Code that you should not touch if you don't know what you're doing.\n",
        "#Feel free to view it, though.\n",
        "\n",
        "\n",
        "#Query the OpenAI completions API\n",
        "response = client.completions.create(\n",
        "  model=model,\n",
        "  prompt=prompt,\n",
        "  temperature=temperature,\n",
        "  max_tokens=max_tokens,\n",
        ")\n",
        "\n",
        "#Print the response\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "1ITZeptzss0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What you should observe above:**\n",
        "\n",
        "* gpt-3.5-turbo-instruct obeys instructions better than davinci-002, but the generated ideas can be more repetitive and generic.\n",
        "* Especially with davinci-002, quality decreases over time if you generate many ideas in one go. This is because each generated token becomes part of the prompt for the next generated token. In other words, *the generated ideas become new few-shot examples for the next ideas*, and random errors gradually accumulate and make the output diverge. This is common to all LLMs, just more noticeable with base models like davinci-002.\n",
        "* With davinci-002, reducing the temperature can help in balancing diversity and quality. The parameter controls how random the generation is. Temperature=1 is the default, where the model picks every next token randomly according to their probabilities. Temperature=0 means the model only picks the most probable tokens and disregards any alternatives.\n",
        "\n",
        "**Why is davinci-002 less reliable but more diverse?**\n",
        "\n",
        "The davinci-002 is the so-called \"base\" OpenAI model trained on very large and diverse data. gpt-3.5-turbo-instruct has been finetuned to follow instructions, which tends to increase generation quality but reduce diversity and produce the recognizably boring voice of ChatGPT. Finetuning is typically formulated as an optimization problem for which the optimal solution is to always give the single best answer to a particular request or question. The finetuning datasets are also smaller, which inevitably limits diversity.\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "Different models have different strengths and weaknesses. If correctness is a priority - for example, in generating code - you typically need a finetuned model. But if the finetuned model's output is too boring or vanilla, it may be good to try a base model."
      ],
      "metadata": {
        "id": "mXjRGtIquS7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Few-shot prompting in chat mode\n",
        "Try the above, but with a chat model such as ChatGPT, Anthropic Claude, or Google Gemini.\n",
        "\n",
        "You can access these in various ways for free:\n",
        "- ChatGPT is available in the Edge browser if you are signed into a Microsoft Account\n",
        "- ChatGPT also has a free trial if accessed via https://chat.openai.com\n",
        "- Google Gemini is available here in Colab (top-right corner of the window) and at https://gemini.google.com/app, although this is only the less capable variant. Gemini Pro requires a subscription.\n",
        "- Claude is available for free at: https://www.anthropic.com/pricing\n",
        "\n",
        "In this case, you have to format your input a bit differently, such as:\n",
        "\n",
        "```\n",
        "Please give me a novel and experimental indie game idea.\n",
        "\n",
        "Here are some examples of how to describe the idea in a way that clearly explains the core mechanics and/or design hook:\n",
        "\n",
        "- An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "- A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "```\n",
        "\n",
        "**What you should observe:**\n",
        "\n",
        "Again, each model has its own default flavor/tone of writing. Typically, as the chat models are heavily finetuned, it can be hard to break away from their default tone. Thus, using a non-chat base model can be a better alternative, depending on the output you want."
      ],
      "metadata": {
        "id": "JzTk0MRbeV2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Craft your own few-shot examples\n",
        "\n",
        "Above, the few-shot examples describe the indie games Superhot and Baba Is You in a way that makes the core mechanics clear. These award-winning indie games are some of my own personal favorites.\n",
        "\n",
        "Now, your task is to replace the examples with descriptions of some other games, either real or fictional.\n",
        "\n",
        "If your mind draws a blank, here are some sources for finding good examples:\n",
        "- The Steam store descriptions of Steam's [hidden gems](https://steam250.com/hidden_gems). For each game, click on the Steam symbol that takes you to the store page.\n",
        "- The descriptions of [Independent Games Festival finalists](https://igf.com/finalists-and-winners). For each finalist game, click to see the IGF entry page, e.g. [https://igf.com/entry/2024/1000xresist](https://igf.com/entry/2024/1000xresist)\n",
        "\n",
        "Instead of games, you can also try creating other texts. Note that you will also need to rewrite the prompt start, accordingly. Here are some sources for examples:\n",
        "- Generating 6-word stories based on examples from [r/sixwordstories](https://www.reddit.com/r/sixwordstories/top/?t=all)\n",
        "- To get inspiration for longer stories, start by generating good opening sentences using [these examples](https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books). Note that you should also edit the prompt start.\n",
        "- More inspiration for stories and aforisms can be found via Kindle's [most highlighted passages](https://www.amazon.com/amazonbookreview/read/B09KVGXNQQ?)"
      ],
      "metadata": {
        "id": "Lg9uKDoiM7Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "#Parameters that you can edit\n",
        "model=\"gpt-3.5-turbo-instruct\" # @param [\"davinci-002\", \"gpt-3.5-turbo-instruct\"] {allow-input: false}\n",
        "temperature=1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "max_tokens=200 # @param {type:\"slider\", min:10, max:1000, step:10}\n",
        "\n",
        "prompt=\"\"\"A list of novel and experimental indie game ideas:\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "###################################################################\n",
        "#Code that you should not touch if you don't know what you're doing.\n",
        "#Feel free to view it, though.\n",
        "\n",
        "\n",
        "#Query the OpenAI completions API\n",
        "response = client.completions.create(\n",
        "  model=model,\n",
        "  prompt=prompt,\n",
        "  temperature=temperature,\n",
        "  max_tokens=max_tokens,\n",
        ")\n",
        "\n",
        "#Print the response\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "JDWXULB5vfGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Curate your generated content and examples\n",
        "\n",
        "Try this:\n",
        "- Generate at least 10 ideas or stories\n",
        "- Pick the best generation(s) and add them to your few-shot examples\n",
        "- Repeat the steps above until you have between 5 and 10 examples. Do you notice an increase in the quality of generations?\n"
      ],
      "metadata": {
        "id": "QGDtES24bBZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "#Parameters that you can edit\n",
        "model=\"gpt-3.5-turbo-instruct\" # @param [\"davinci-002\", \"gpt-3.5-turbo-instruct\"] {allow-input: false}\n",
        "temperature=1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "max_tokens=200 # @param {type:\"slider\", min:10, max:1000, step:10}\n",
        "\n",
        "prompt=\"\"\"A list of novel and experimental indie game ideas:\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "###################################################################\n",
        "#Code that you should not touch if you don't know what you're doing.\n",
        "#Feel free to view it, though.\n",
        "\n",
        "\n",
        "#Query the OpenAI completions API\n",
        "response = client.completions.create(\n",
        "  model=model,\n",
        "  prompt=prompt,\n",
        "  temperature=temperature,\n",
        "  max_tokens=max_tokens,\n",
        ")\n",
        "\n",
        "#Print the response\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "2XytHv8ow2yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Guide the generation in a specific direction\n",
        "Try combining few-shot examples with the following:\n",
        "- Rewrite the prompt start to be more specific, e.g., add adjectives\n",
        "- Guide the new generations by writing the start of the generated text by yourself\n",
        "- Test both base models and finetuned models (davinci-002 and gpt-3.5-turbo-instruct)\n",
        "\n",
        "For example, you can try the prompt below and then rewrite the last line by combining some other game genre and emotion such as \"Space Invaders but about heartbreak\""
      ],
      "metadata": {
        "id": "yW9mgnUObr51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "#Parameters that you can edit\n",
        "model=\"gpt-3.5-turbo-instruct\" # @param [\"davinci-002\", \"gpt-3.5-turbo-instruct\"] {allow-input: false}\n",
        "temperature=1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "max_tokens=100 # @param {type:\"slider\", min:10, max:1000, step:10}\n",
        "\n",
        "prompt=\"\"\"A list of novel, witty, and experimental indie game ideas that clearly describe the core design hook:\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game about love\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "###################################################################\n",
        "#Code that you should not touch if you don't know what you're doing.\n",
        "#Feel free to view it, though.\n",
        "\n",
        "\n",
        "#Query the OpenAI completions API\n",
        "response = client.completions.create(\n",
        "  model=model,\n",
        "  prompt=prompt,\n",
        "  temperature=temperature,\n",
        "  max_tokens=max_tokens,\n",
        ")\n",
        "\n",
        "#Print the response\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "mJvbPIpBvmlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Ef6F48YRYW"
      },
      "source": [
        "### Combinatorial few-shot prompting\n",
        "A common creativity method for humans is to pick random elements and recombine them. For example, the VNA game ideation method combines a random Verb, Noun, and Adjective.\n",
        "\n",
        "This can help one think outside the box, and it can also be useful to force an LLM to generate more diverse ideas, especially when using a finetuned model that by default produces high quality but less diverse generations.\n",
        "\n",
        "Here, we extend few-shot prompting with randomly combined modifiers: We prompt game ideas by *systematically combining genres and mechanics*.\n",
        "\n",
        "**Exercises:**\n",
        "* Run the code a few times and try different models. Try adding a third modifier, e.g., \"Single Player, Multiplayer\".\n",
        "\n",
        "* Change the prompt start, modifiers, and examples to generate different kind of games. For example, you could try combining classic games such as Space Invaders and Pac Man with different emotions such as sadness, love, grief, heartbreak. If it's hard to come up with the few-shot examples, you can try prompting multiple times with less or no examples and then add the best results as examples.\n",
        "\n",
        "* Generate story ideas that combine character archetypes from https://thescriptlab.com/blogs/38406-250-character-archetypes-to-use-in-your-screenplay/ with a type of conflict the characters face.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###################################################################\n",
        "#Parameters that you can edit\n",
        "model=\"gpt-3.5-turbo-instruct\" # @param [\"davinci-002\", \"gpt-3.5-turbo-instruct\"] {allow-input: false}\n",
        "temperature=1 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "max_tokens=200 # @param {type:\"slider\", min:10, max:1000, step:10}\n",
        "\n",
        "prompt=\"\"\"A list of novel, witty, and experimental indie game ideas that clearly describe the core design hook:\n",
        "\n",
        "---\n",
        "\n",
        "FPS + Time manipulation: An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "Puzzle + Rule manipulation: A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "Platformer + Camera manipulation: An FPS puzzle game with a \"portal gun\" that can create portals between two flat planes. For example, the player can create one portal on the floor and the second on the ceiling and push an object so that it falls down the floor portal and drops from the ceiling portal on top of some target.\n",
        "\n",
        "---\n",
        "\n",
        "Dating sim + Time manipulation:\n",
        "\"\"\"\n",
        "\n",
        "###################################################################\n",
        "#Code that you should not touch if you don't know what you're doing.\n",
        "#Feel free to view it, though.\n",
        "\n",
        "\n",
        "#Query the OpenAI completions API\n",
        "response = client.completions.create(\n",
        "  model=model,\n",
        "  prompt=prompt,\n",
        "  temperature=1,\n",
        "  max_tokens=500,\n",
        ")\n",
        "\n",
        "#Print the response\n",
        "print(response.choices[0].text)\n"
      ],
      "metadata": {
        "id": "FdL7L91YxoXZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OvXpXUB5fEwt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}