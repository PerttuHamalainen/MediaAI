{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c2031d7e",
      "metadata": {
        "id": "c2031d7e"
      },
      "source": [
        "# Exercise: CLIP-Based Image/Text Retrieval and Synthesis\n",
        "\n",
        "In this exercise, we will build some intuitive understanding of how a general-purpose image/text correspondence models such as OpenAI's Contrastive Language-Image Pretraining (CLIP) model and diffusion models can be used for generative and creative purposes.\n",
        "\n",
        "#### Learning goals:\n",
        "\n",
        "* Use CLIP encoders/decoders to perform image retrieval and synthesis\n",
        "* Examine the use of diffusion model for image generation\n",
        "\n",
        "#### After you've read, run, and understood the provided code, perform these tasks:\n",
        "\n",
        "* Easy: change the prompt for image synthesis until you get an interesting image.\n",
        "    * You can share your image (as long as appropriate) on class Discord.\n",
        "* Easy: using your own text samples, perform latent blending between two text samples and visualize the resulting image using image synthesis.\n",
        "    * Suggestion: See for yourself that latent math like \"king\" - \"man\" + \"woman\" = \"queen\" holds\n",
        "    * Suggestion: try blending a \"content prompt\" with a \"style prompt\", e.g. \"an aerial view of a city\" + \"a Van Gogh painting\"\n",
        "    * Again, feel free to share your results!\n",
        "* Medium: modify the code to perform image search (by text and image)\n",
        "    * Download the COCO Val2017 dataset (http://images.cocodataset.org/zips/val2017.zip)\n",
        "    * Find an image within the dataset, whose CLIP-embedding is closest to the following image (https://i.imgur.com/S0iR4Zr.png):\n",
        "    <div>\n",
        "    <img src=\"https://i.imgur.com/S0iR4Zr.png\" width=\"300\"/>\n",
        "    </div>\n",
        "    * Find an image within the dataset, whose CLIP-embedding is closest to the following text prompt:\n",
        "```\n",
        "man eating pie\n",
        "```\n",
        "* Easy (optional): if your image/text retrieval is too slow, pre-compute embeddings to accelerate things.\n",
        "* Easy: using your own text samples, perform latent blending between two text samples and visualize the resulting image using image retrieval.\n",
        "* Easy: using your own text samples, perform latent blending between two text samples and search for the best sample text describing the blended text among Flickr8k captions (https://www.dropbox.com/s/4dgs7e0r3ypaqus/just_captions.txt?dl=1).\n",
        "* Medium: perform latent blending between two image samples from COCO Val2017. Search for the best caption describing the blended image among Flickr8k captions.\n",
        "* Hard: Use a CLIP encoder to create a \"style embedding\" from an image or a text, and then perform a style transfer on any image of your choice.\n",
        "* Hard: Fork the clipcoders repository (https://github.com/namheegordonkim/clipcoders) and add another decoder that performs supersampling diffusion from 256x256 images to 512x512 images. (See https://github.com/crowsonkb/v-diffusion-pytorch/issues/10)\n",
        "* Project idea: Fine-tune CLIP with an image caption dataset of your choice. Visualize the fine-tuned results with image retrieval and synthesis (for synthesis, use a classifier-guided diffusion decoder)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49e6164d",
      "metadata": {
        "id": "49e6164d"
      },
      "source": [
        "## Dependencies\n",
        "\n",
        "We will use a customized encoder/decoder objects to make our code easier to read. After understanding what's going on, you are most welcome to read the source code, as understanding the array/tensor manipulations underneath will be more useful for training and scaling up the operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5465b740",
      "metadata": {
        "id": "5465b740",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# CLIP\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "# Custom Encoder-Decoder wrappers\n",
        "!pip install git+https://github.com/namheegordonkim/clipcoders\n",
        "\n",
        "# Demo images\n",
        "!wget https://i.imgur.com/eB1otqV.png -O child.png\n",
        "!wget https://i.imgur.com/YETS1O1.png -O two-dogs-0.png\n",
        "!wget https://i.imgur.com/H4tTA0r.png -O two-dogs-1.png\n",
        "!wget https://i.imgur.com/6mpZTzW.jpg -O cat.jpg\n",
        "!wget https://i.imgur.com/S0iR4Zr.png -O cat2.png"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c1cebf",
      "metadata": {
        "id": "c1c1cebf"
      },
      "source": [
        "As usual, we import necessary dependencies at the top of the notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b2bd344",
      "metadata": {
        "id": "1b2bd344"
      },
      "outputs": [],
      "source": [
        "import clip\n",
        "import torch\n",
        "import numpy as np\n",
        "from clipcoders.encoders import CLIPTextEncoder, CLIPImageEncoder\n",
        "from clipcoders.decoders import ClassifierFreeGuidanceDecoder\n",
        "from clipcoders.diffusion.models import get_model\n",
        "from clipcoders.diffusion.utils import to_pil_image, from_pil_image\n",
        "from PIL import Image\n",
        "from torch.nn import functional as F\n",
        "from torchvision.transforms import functional as TF\n",
        "from IPython.core.display import display, HTML\n",
        "from glob import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c04c047f",
      "metadata": {
        "id": "c04c047f"
      },
      "source": [
        "### Load a Pre-Trained CLIP Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dfec07f",
      "metadata": {
        "id": "1dfec07f"
      },
      "outputs": [],
      "source": [
        "clip_model = clip.load(\"ViT-B/16\")[0]\n",
        "clip_model.eval().requires_grad_(False).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dda290e6",
      "metadata": {
        "id": "dda290e6"
      },
      "source": [
        "### Make CLIP-Based Encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60c5ff7c",
      "metadata": {
        "id": "60c5ff7c"
      },
      "outputs": [],
      "source": [
        "clip_text_encoder = CLIPTextEncoder(\n",
        "    clip_model=clip_model,\n",
        "    device=device\n",
        ")\n",
        "clip_image_encoder = CLIPImageEncoder(\n",
        "    clip_model=clip_model,\n",
        "    device=device,\n",
        "    cutn=16,\n",
        "    cut_pow=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be579a2c",
      "metadata": {
        "id": "be579a2c"
      },
      "source": [
        "What does a CLIP-based text encoder do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64405da6",
      "metadata": {
        "id": "64405da6"
      },
      "outputs": [],
      "source": [
        "sample_text_embedding = clip_text_encoder.encode(\"here is a sample text.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0270a05",
      "metadata": {
        "id": "a0270a05"
      },
      "outputs": [],
      "source": [
        "print(sample_text_embedding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8d391d0",
      "metadata": {
        "id": "b8d391d0"
      },
      "source": [
        "What does a CLIP-based image encoder do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c28006f",
      "metadata": {
        "id": "7c28006f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "sample_image = Image.open(\"cat.jpg\")\n",
        "sample_image_tensor = from_pil_image(sample_image).unsqueeze(0).to(device)\n",
        "sample_image_embedding = clip_image_encoder.encode(sample_image_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42988dee",
      "metadata": {
        "id": "42988dee"
      },
      "outputs": [],
      "source": [
        "print(sample_image_embedding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7084778",
      "metadata": {
        "id": "e7084778"
      },
      "source": [
        "## Measuring Latent Similarity with Embeddings\n",
        "\n",
        "Why would one want to use image and text encoders? Image-text correspondence models like CLIP establishes a latent space where images and texts can be compared in terms of semantic content. Below, we provide a simple example."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c9d7a55",
      "metadata": {
        "id": "2c9d7a55"
      },
      "source": [
        "Consider the following three image-text pairs:\n",
        "\n",
        "<div>\n",
        "    <table class=\"tg\">\n",
        "    <thead>\n",
        "      <tr>\n",
        "        <th>Index</th>\n",
        "        <th class=\"tg-0pky\">Image</th>\n",
        "        <th class=\"tg-0pky\">Text</th>\n",
        "      </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "      <tr>\n",
        "        <td>0</td>\n",
        "        <td class=\"tg-fymr\"><img src=\"https://i.imgur.com/YETS1O1.png\" width=\"300\"/></td>\n",
        "        <td class=\"tg-fymr\">Two dogs of different breeds looking at each other on the road.</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>1</td>\n",
        "        <td class=\"tg-0pky\"><img src=\"https://i.imgur.com/H4tTA0r.png\" width=\"300\"/></td>\n",
        "        <td class=\"tg-0pky\">Two dogs playing together on a beach.</td>\n",
        "      </tr>\n",
        "      <tr>\n",
        "        <td>2</td>\n",
        "        <td class=\"tg-0pky\"><img src=\"https://i.imgur.com/eB1otqV.png\" width=\"300\"/></td>\n",
        "        <td class=\"tg-0pky\">A child biting into a baked good.</td>\n",
        "      </tr>\n",
        "    </tbody>\n",
        "    </table>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adc993aa",
      "metadata": {
        "id": "adc993aa"
      },
      "source": [
        "Images 0 and 1 are similar in concept, and image 2 is very different from the others. How does CLIP help us quantify this difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7dd7b64",
      "metadata": {
        "id": "c7dd7b64"
      },
      "outputs": [],
      "source": [
        "# A distance metric to be used in latent spaces\n",
        "def spherical_dist(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2).mean()\n",
        "\n",
        "# Sometimes, you will want group-to-group distances\n",
        "def spherical_dists(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "# To make images more uniform-sized\n",
        "def resize_and_center_crop(image, size):\n",
        "    fac = max(size[0] / image.size[0], size[1] / image.size[1])\n",
        "    image = image.resize((int(fac * image.size[0]), int(fac * image.size[1])), Image.LANCZOS)\n",
        "    return TF.center_crop(image, size[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292a8fc7",
      "metadata": {
        "id": "292a8fc7"
      },
      "outputs": [],
      "source": [
        "# To quantify the difference among pairs of images\n",
        "# Prepare image tensors\n",
        "two_dogs_0_url = \"https://i.imgur.com/YETS1O1.png\"\n",
        "two_dogs_1_url = \"https://i.imgur.com/H4tTA0r.png\"\n",
        "child_url = \"https://i.imgur.com/eB1otqV.png\"\n",
        "\n",
        "two_dogs_0_filename = \"two-dogs-0.png\"\n",
        "two_dogs_1_filename = \"two-dogs-1.png\"\n",
        "child_filename = \"child.png\"\n",
        "\n",
        "two_dogs_0_image = resize_and_center_crop(Image.open(two_dogs_0_filename).convert(\"RGB\"), (640, 480))\n",
        "two_dogs_0_image_tensor = from_pil_image(two_dogs_0_image).unsqueeze(0).to(device)\n",
        "two_dogs_1_image = resize_and_center_crop(Image.open(two_dogs_1_filename).convert(\"RGB\"), (640, 480))\n",
        "two_dogs_1_image_tensor = from_pil_image(two_dogs_1_image).unsqueeze(0).to(device)\n",
        "child_image = resize_and_center_crop(Image.open(child_filename).convert(\"RGB\"), (640, 480))\n",
        "child_image_tensor = from_pil_image(child_image).unsqueeze(0).to(device)\n",
        "\n",
        "# Prepare image embeddings\n",
        "two_dogs_0_image_embedding = clip_image_encoder.encode(two_dogs_0_image_tensor)\n",
        "two_dogs_1_image_embedding = clip_image_encoder.encode(two_dogs_1_image_tensor)\n",
        "child_image_embedding = clip_image_encoder.encode(child_image_tensor)\n",
        "\n",
        "# Finally, compute distances between embeddings\n",
        "display(HTML(\n",
        "    f\"\"\"\n",
        "    <table>\n",
        "    <tr>\n",
        "    <td><img src={two_dogs_0_url} width=200px /></td>\n",
        "    <td><img src={two_dogs_0_url} width=200px /></td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_image_embedding, two_dogs_0_image_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td><img src={two_dogs_0_url} width=200px /></td>\n",
        "    <td><img src={two_dogs_1_url} width=200px /></td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_image_embedding, two_dogs_1_image_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td><img src={two_dogs_0_url} width=200px /></td>\n",
        "    <td><img src={child_url} width=200px /></td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_image_embedding, child_image_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td><img src={two_dogs_1_url} width=200px /></td>\n",
        "    <td><img src={child_url} width=200px /></td>\n",
        "    <td>dist={spherical_dist(two_dogs_1_image_embedding, child_image_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    </table>\n",
        "    \"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c9f180e",
      "metadata": {
        "id": "6c9f180e"
      },
      "outputs": [],
      "source": [
        "# To quantify the difference among pairs of texts\n",
        "# Prepare texts\n",
        "two_dogs_0_text = \"Two dogs of different breeds looking at each other on the road.\"\n",
        "two_dogs_1_text = \"Two dogs playing together on a beach.\"\n",
        "child_text = \"A child biting into a baked good.\"\n",
        "\n",
        "# Prepare text embeddings\n",
        "two_dogs_0_text_embedding = clip_text_encoder.encode(two_dogs_0_text)\n",
        "two_dogs_1_text_embedding = clip_text_encoder.encode(two_dogs_1_text)\n",
        "child_text_embedding = clip_text_encoder.encode(child_text)\n",
        "\n",
        "# Compute distance between embeddings\n",
        "display(HTML(\n",
        "    f\"\"\"\n",
        "    <table>\n",
        "    <tr>\n",
        "    <td>{two_dogs_0_text}</td>\n",
        "    <td>{two_dogs_0_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_text_embedding, two_dogs_0_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td>{two_dogs_0_text}</td>\n",
        "    <td>{two_dogs_1_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_text_embedding, two_dogs_1_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td>{two_dogs_0_text}</td>\n",
        "    <td>{child_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_text_embedding, child_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td>{two_dogs_1_text}</td>\n",
        "    <td>{child_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_1_text_embedding, child_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    </table>\n",
        "    \"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed8d7aec",
      "metadata": {
        "id": "ed8d7aec",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# To compute distance between images and texts\n",
        "display(HTML(\n",
        "    f\"\"\"\n",
        "    <table>\n",
        "    <tr>\n",
        "    <td><img src={two_dogs_0_url} width=200px /></td>\n",
        "    <td>{two_dogs_0_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_image_embedding, two_dogs_0_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td><img src={two_dogs_0_url} width=200px /></td>\n",
        "    <td>{two_dogs_1_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_image_embedding, two_dogs_1_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td><img src={two_dogs_0_url} width=200px /></td>\n",
        "    <td>{child_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_image_embedding, child_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td><img src={two_dogs_1_url} width=200px /></td>\n",
        "    <td>{two_dogs_1_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_1_image_embedding, two_dogs_1_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td><img src={two_dogs_1_url} width=200px /></td>\n",
        "    <td>{two_dogs_0_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_1_image_embedding, two_dogs_0_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    \n",
        "    <tr>\n",
        "    <td><img src={two_dogs_1_url} width=200px /></td>\n",
        "    <td>{child_text}</td>\n",
        "    <td>dist={spherical_dist(two_dogs_0_image_embedding, child_text_embedding):.3f}</td>\n",
        "    </tr>\n",
        "    </table>\n",
        "    \"\"\"\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a7503e",
      "metadata": {
        "id": "37a7503e"
      },
      "source": [
        "## CLIP-Based Image Retrieval from Text\n",
        "\n",
        "Since CLIP's image embeddings and text embeddings can be used to compute similarities across these domains, we can do something akin to Google's image search, where a text input is matched with images. Generally, these steps are involved:\n",
        "\n",
        "* Encode text\n",
        "* Encode images\n",
        "* Get distance between text and each image\n",
        "* Return the image with the lowest distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84f3169",
      "metadata": {
        "id": "a84f3169"
      },
      "outputs": [],
      "source": [
        "# This is our \"database\" of images\n",
        "images = [two_dogs_0_image, two_dogs_1_image, child_image]\n",
        "\n",
        "# A function to make it easier to do image retrieval\n",
        "def get_image_from_prompt(prompt, images):\n",
        "    # to prevent gradient computation from clogging memory\n",
        "    with torch.no_grad():\n",
        "        prompt_embeddig = clip_text_encoder.encode(prompt)\n",
        "        image_embeddings = [clip_image_encoder.encode(from_pil_image(image).unsqueeze(0).to(device)) for image in images]\n",
        "        dists = np.array([spherical_dist(prompt_embeddig, embedding).mean().cpu().detach().numpy() for embedding in image_embeddings])\n",
        "    return images[np.argmin(dists)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd4d1f9",
      "metadata": {
        "id": "6bd4d1f9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "get_image_from_prompt(\"beach\", images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8368b9e6",
      "metadata": {
        "id": "8368b9e6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "get_image_from_prompt(\"child\", images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "962640b8",
      "metadata": {
        "id": "962640b8",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "get_image_from_prompt(\"fighting\", images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0728c0f",
      "metadata": {
        "id": "e0728c0f"
      },
      "source": [
        "## Exercise: Image Retrieval from COCO Val2017 Dataset with Text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bbf83c63",
      "metadata": {
        "id": "bbf83c63",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "!wget http://images.cocodataset.org/zips/val2017.zip\n",
        "!unzip val2017.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "effedc7a",
      "metadata": {
        "id": "effedc7a"
      },
      "outputs": [],
      "source": [
        "class MyImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Makes it easy to interact with the downloaded dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, root):\n",
        "        self.filenames = sorted(glob(f\"{root}/*.jpg\"))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return resize_and_center_crop(Image.open(self.filenames[index]).convert(\"RGB\"), (640, 480))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qSjhKEuROY0L",
      "metadata": {
        "id": "qSjhKEuROY0L"
      },
      "outputs": [],
      "source": [
        "image_dataset = MyImageDataset(\"val2017\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb9b3fc",
      "metadata": {
        "id": "beb9b3fc"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2573f2f4",
      "metadata": {
        "id": "2573f2f4"
      },
      "source": [
        "## Exercise: Image Retrieval from COCO Val2017 Dataset with Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f830cf7a",
      "metadata": {
        "id": "f830cf7a"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd1897c",
      "metadata": {
        "id": "8cd1897c"
      },
      "source": [
        "## Exercise: Finding Image for Latent Blending of Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1252a4e6",
      "metadata": {
        "id": "1252a4e6"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dafc6cb",
      "metadata": {
        "id": "3dafc6cb"
      },
      "source": [
        "## Exercise: Finding Text for Latent Blending of Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea754a3",
      "metadata": {
        "id": "3ea754a3"
      },
      "outputs": [],
      "source": [
        "!wget https://www.dropbox.com/s/4dgs7e0r3ypaqus/just_captions.txt?dl=1 -O just_captions.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c00a3e3",
      "metadata": {
        "id": "7c00a3e3"
      },
      "outputs": [],
      "source": [
        "class MyTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Makes it easy to interact with the downloaded dataset\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, file):\n",
        "        with open(file, \"r\") as f:\n",
        "            self.lines = f.read().split(\"\\n\")\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self.lines[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e65c03",
      "metadata": {
        "id": "f8e65c03"
      },
      "outputs": [],
      "source": [
        "text_dataset = MyTextDataset(\"just_captions.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd3d89f",
      "metadata": {
        "id": "3bd3d89f"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de3b8952",
      "metadata": {
        "id": "de3b8952"
      },
      "source": [
        "## Exercise: Finding Text for Latent Blending of Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d987e7c2",
      "metadata": {
        "id": "d987e7c2"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b9100f",
      "metadata": {
        "id": "d8b9100f"
      },
      "source": [
        "## CLIP-Based Image Synthesis \n",
        "\n",
        "While *retrieval* has to do with returning matching images that are actually in the dataset, *synthesis* is rendering the embedding into a brand-new image. In 2021, some critical breakthroughs were made with the development of CLIP and diffusion models. Below, we will use diffusion-based decoder to render a prompt into an image and appreciate the beauty and power of the AI-generated art.\n",
        "\n",
        "### Load a Pre-Trained Diffusion Model (CLIP-Conditioned)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19fe6172",
      "metadata": {
        "id": "19fe6172",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!wget https://v-diffusion.s3.us-west-2.amazonaws.com/cc12m_1_cfg.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9b68806",
      "metadata": {
        "id": "e9b68806",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "diffusion_model = get_model(\"cc12m_1_cfg\")()\n",
        "state_dict = torch.load(\"cc12m_1_cfg.pth\", map_location=\"cpu\")\n",
        "diffusion_model.load_state_dict(state_dict)\n",
        "if device.type == 'cuda':\n",
        "    diffusion_model = diffusion_model.half()\n",
        "diffusion_model.eval().requires_grad_(False).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fc97e8e",
      "metadata": {
        "id": "1fc97e8e"
      },
      "source": [
        "### Instantiate a Diffusion-Based Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9532b7e1",
      "metadata": {
        "id": "9532b7e1"
      },
      "outputs": [],
      "source": [
        "decoder = ClassifierFreeGuidanceDecoder(\n",
        "    diffusion_model=diffusion_model,\n",
        "    device=device,\n",
        "    n_steps=500\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed962fd",
      "metadata": {
        "id": "1ed962fd",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Sensitive to random seed\n",
        "prompt = \"Two dogs of different breeds looking at each other on the road.\"\n",
        "prompt_embeddig = clip_text_encoder.encode(prompt)\n",
        "image = decoder.decode(prompt_embeddig)\n",
        "to_pil_image(image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f964db",
      "metadata": {
        "id": "85f964db"
      },
      "source": [
        "The results don't look so good. Why would that be the case? Some possible explanations:\n",
        "\n",
        "1. The dataset used to pre-train the diffusion model did not have enough examples of dogs\n",
        "2. Our prompt did not emphasize how realistic the dogs must look\n",
        "\n",
        "Our particular CLIP-guided diffusion pipeline is not very suitable for realism. In December 2021, OpenAI released another CLIP-based image synthesis model named GLIDE (https://github.com/openai/glide-text2im), which addresses this bottleneck.\n",
        "\n",
        "However, our image synthesis is very powerful when it comes to generating art. See below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d72f81",
      "metadata": {
        "id": "92d72f81"
      },
      "outputs": [],
      "source": [
        "# Sensitive to random seed\n",
        "prompt = \"magical ruins at dawn. watercolor. by artists on artstation.\"\n",
        "prompt_embeddig = clip_text_encoder.encode(prompt)\n",
        "image = decoder.decode(prompt_embeddig)\n",
        "to_pil_image(image[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5722bf84",
      "metadata": {
        "id": "5722bf84"
      },
      "source": [
        "## Exercise: Latent Blending of Prompts, Visualized via Image Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7554f8b",
      "metadata": {
        "id": "d7554f8b"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "exercise-clip-guided-things-solutions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}