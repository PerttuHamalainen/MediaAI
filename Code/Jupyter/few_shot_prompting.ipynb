{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s3mPrBIHhts"
      },
      "source": [
        "## Tutorial and exercises: Few-Shot prompting and semantic embeddings\n",
        "*This Colab notebook is part of the [AI for media, art, and design](https://github.com/PerttuHamalainen/MediaAI) course of Aalto University* and also used on the [Art of Writing](https://aalto.my.site.com/courses/s/course/a057T0000043wJBQAY/art-of-writing?language=en_US) course.\n",
        "\n",
        "This notebook demonstrates text generation using Few-Shot prompting and provides exercises to extend notebook. Solutions to the exercises are also provided, but the code is hidden by default.\n",
        "\n",
        "**What is few-shot prompting?**\n",
        "\n",
        "Few-shot prompting means that one includes *high-quality examples* of desired outputs in the LLM prompt. This can greatly improve the generated results. Similar to explaining things to a human, using concrete examples is often the most *efficient and precise way* to explain what one wants.\n",
        "\n",
        "*Finding/writing the few-shot examples is a key skill to practise* in AI-assisted creative writing. Typically, this includes both writing example text yourself, and browsing and/or scraping the Internet for examples.\n",
        "\n",
        "**Learning goals:**\n",
        "\n",
        "* Practice producing the few-shot examples and learn to anticipate how different examples change the results\n",
        "* Compare different LLMs and understand the differences between base models and finetuned models, as well as continuation models and chat models.\n",
        "* Practice visualizing and exploring a large number of generations using semantic embeddings. The same approach can be applied to exploring other text datasets such as customer support messages or social media posts.\n",
        "* Practice combinatorial few-shot prompting. For example, we will create game ideas that systematically explore combinations of game genres and mechanics\n",
        "\n",
        "**How to use:**\n",
        "\n",
        "* Make sure you have an OpenAI account. You can create an OpenAI account for free at https://platform.openai.com/signup. The initial free quota you get with the account should be enough for the exercises of this notebook.\n",
        "* Proceed through this notebook on your own pace. If you have zero background in programming, you might want to only do the Part 1 below.\n",
        "\n",
        "**New to Colab notebooks?**\n",
        "\n",
        "Colab notebooks are browser-based learning environments consisting of *cells* that include either text or code. The code is executed in a Google virtual machine instead of your own computer. You can run code cell-by-cell (click the \"play\" symbol of each code cell), and selecting \"Run all\" as instructed above is usually the first step to verify that everything works. For more info, see Google's [Intro video](https://www.youtube.com/watch?v=inN8seMm7UI) and [curated example notebooks](https://colab.google/notebooks/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Few-shot prompting in OpenAI Playground\n",
        "In this part, we practice few-shot prompting in [OpenAI Playground](https://platform.openai.com/playground/), **without any Python code.**\n",
        "\n",
        "**To work on this part, click on the \">\" symbol on the left** and follow the instructions.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yilz7X-WNYeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Exercise: Generating game ideas and comparing different LLMs\n",
        "\n",
        "1. Open [OpenAI Playground](https://platform.openai.com/playground/) in a new tab of your browser.\n",
        "2. Select \"Completions\" from the left. This allows you to use non-chat models that simply continue any text you give.\n",
        "3. Select \"gpt-3.5-turbo-instruct\" from the \"Model\" menu on the right. This is the text continuing version of ChatGPT-3.5.\n",
        "3. Copy-paste the following prompt into the big textbox in the middle of the playground view:\n",
        "\n",
        "```\n",
        "A list of experimental indie game ideas:\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "```\n",
        "\n",
        "4. Click the \"Submit\" button at the bottom of the playground view to generate new list items. Increase the \"Maximum length\" parameter if needed.\n",
        "5. Click the \"Regenerate\" (two circular arrows) button 5 times to get alternative generations.\n",
        "6. Select \"davinci-002\" from the \"Model\" menu and click \"Regenerate\" 5 times. Davinci-002 is the non-finetuned base model.\n",
        "7. Change the \"Temperature\" parameter (on the right) to a smaller value, such as 0.7 or 0.5. Click \"Regenerate\" 5 times.\n",
        "\n",
        "\n",
        "**What you should observe:**\n",
        "\n",
        "* gpt-3.5-turbo-instruct obeys instructions better than davinci-002, but the generated ideas can be more repetitive and generic.\n",
        "* Especially with davinci-002, quality decreases with each idea generated in one go, as the generated ideas become new few-shot examples for the next ideas and random errors gradually accumulate and throw the model off the rails.\n",
        "* With davinci-002, reducing the temperature can help in balancing diversity and quality. The parameter controls how random the generation is. Temperature=1 is the default, where the model picks every next token randomly according to their probabilities. Temperature=0 means the model only picks the most probable tokens and disregards any alternatives.\n",
        "\n",
        "**Why is davinci-002 less reliable but more diverse?**\n",
        "\n",
        "The davinci-002 is the so-called \"base\" OpenAI model trained on very large and diverse data. gpt-3.5-turbo-instruct has been finetuned to follow instructions, which tends to increase generation quality but reduce diversity and produce the recognizably boring voice of ChatGPT. Finetuning is typically formulated as an optimization problem for which the optimal solution is to always give the single best answer to a particular request or question. The finetuning datasets are also smaller, which inevitably limits diversity.\n",
        "\n",
        "**Key takeaway:**\n",
        "\n",
        "Different models have different strengths and weaknesses. If correctness is a priority - for example, in generating code - you typically need a finetuned model. But if the finetuned model's output is too boring or vanilla, it may be good to try the base model."
      ],
      "metadata": {
        "id": "Ng0Ik1qnrF85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Few-shot prompting in chat mode\n",
        "Try the above, but with a chat model such as ChatGPT, Anthropic Claude, or Google Gemini.\n",
        "\n",
        "In the OpenAI Playground, you can also select \"Chat\" from the left and input your \"user\" messages at the bottom of the view.\n",
        "\n",
        "In this case, you have to format your input a bit differently, such as:\n",
        "\n",
        "```\n",
        "Please give me a novel and experimental indie game idea.\n",
        "\n",
        "Here are some examples of how to describe the idea in a way that clearly explains the core mechanics and/or design hook:\n",
        "\n",
        "- An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "- A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "```\n",
        "\n",
        "**What you should observe:**\n",
        "\n",
        "Again, each model has its own default flavor/tone of writing. Typically, as the chat models are heavily finetuned, it can be hard to break away from the default. Thus, using a non-chat base model can be a better alternative, depending on the output you want."
      ],
      "metadata": {
        "id": "JzTk0MRbeV2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Craft your own few-shot examples\n",
        "\n",
        "Above, the few-shot examples describe the indie games Superhot and Baba Is You in a way that makes the core mechanics clear. These award-winning indie games are some of my own personal favorites.\n",
        "\n",
        "Now, your task is to replace the examples with descriptions of some other games, either real or fictional.\n",
        "\n",
        "If your mind draws a blank, here are some sources for finding good examples:\n",
        "- The Steam store descriptions of Steam's [hidden gems](https://steam250.com/hidden_gems). For each game, click on the Steam symbol that takes you to the store page.\n",
        "- The descriptions of [Independent Games Festival finalists](https://igf.com/finalists-and-winners). For each finalist game, click to see the IGF entry page, e.g. [https://igf.com/entry/2024/1000xresist](https://igf.com/entry/2024/1000xresist)\n",
        "\n",
        "Instead of games, you can also try creating other texts. Note that you will also need to rewrite the prompt start, accordingly. Here are some sources for examples:\n",
        "- Generating 6-word stories based on examples from [r/sixwordstories](https://www.reddit.com/r/sixwordstories/top/?t=all)\n",
        "- To get inspiration for longer stories, start by generating good opening sentences using [these examples](https://www.penguin.co.uk/articles/2022/04/best-first-lines-in-books). Note that you should also edit the prompt start.\n",
        "- More inspiration for stories and aforisms can be found via Kindle's [most highlighted passages](https://www.amazon.com/amazonbookreview/read/B09KVGXNQQ?)"
      ],
      "metadata": {
        "id": "Lg9uKDoiM7Tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Curate your generated content and examples\n",
        "\n",
        "Try this:\n",
        "- Generate at least 10 ideas or stories\n",
        "- Pick the best generation(s) and add them to your few-shot examples\n",
        "- Repeat the steps above until you have between 5 and 10 examples. Do you notice an increase in the quality of generations?\n"
      ],
      "metadata": {
        "id": "QGDtES24bBZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Guide the generation in a specific direction\n",
        "Try combining few-shot examples with the following:\n",
        "- Rewrite the prompt start to be more specific, e.g., add adjectives\n",
        "- Guide the new generations by writing the start of the generated text by yourself\n",
        "- Test both base models and finetuned models (davinci-002 and gpt-3.5-turbo-instruct)\n",
        "\n",
        "For example, you can try this prompt and then rewrite the last line by combining some other game genre and emotion such as \"Space Invaders but about heartbreak\":\n",
        "```\n",
        "A list of novel, witty, and experimental indie game ideas that clearly describe the core design hook:\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game about love\n",
        "```\n"
      ],
      "metadata": {
        "id": "yW9mgnUObr51"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Few-shot prompting using Python code\n",
        "\n",
        "In this part, we explore how adding some automation and post-processing in Python can help getting better results.\n",
        "\n",
        "It is highly recommended that you first do the Part 1 exercises above.\n",
        "\n",
        "**To work on this part, click on the \">\" symbol on the left** and follow the instructions.\n"
      ],
      "metadata": {
        "id": "_qFP8548OUjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting started\n",
        "1. Select \"Run all\" from the Runtime menu. This will run all the code in this notebook so that you can inspect the results.\n",
        "2. Below, enter your OpenAI API key when requested. If you don't have a key, follow [OpenAI's instructions](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)\n",
        "3. Proceed through the notebook, following the exercise instructions.\n",
        "\n",
        "Some exercises will ask you to edit the code. The code the you should not touch is hidden by default."
      ],
      "metadata": {
        "id": "fhAe-DhATr3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from getpass import getpass\n",
        "try: OPENAI_API_KEY\n",
        "except NameError: OPENAI_API_KEY = getpass('Enter OpenAI API key: ')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "abU_u9RXvu44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Do some setup and connect to OpenAI.\n",
        "This part will install and import libraries. It will take a while, but only needs to be done once.\n",
        "\n",
        "A rotating \"stop\" symbol below means the code is executing and you should wait."
      ],
      "metadata": {
        "id": "OvXpXUB5fEwt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX7TEywGVUxX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "#First, install and import code packages\n",
        "#! in the beginning of a Colab code line allows running Linux shell commands\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install umap-learn\n",
        "import tiktoken\n",
        "import time\n",
        "import asyncio\n",
        "import pickle\n",
        "import hashlib\n",
        "import os\n",
        "import openai\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import plotly.express as px\n",
        "import textwrap\n",
        "\n",
        "client=openai.OpenAI(api_key=OPENAI_API_KEY)\n",
        "client_async=openai.AsyncOpenAI(api_key=OPENAI_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To make things easier, this cell defines some code helpers.\n",
        "# source: https://github.com/PerttuHamalainen/LLMCode\n",
        "import json\n",
        "\n",
        "#Colab is already running an asyncio event loop => need this hack for async OpenAI API calling\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#progress bar helper\n",
        "def print_progress_bar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ', printEnd = \"\\r\"):\n",
        "    \"\"\"\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total:\n",
        "        print()\n",
        "\n",
        "#LLM response cache. For reducing API costs, if the exact same prompt has been\n",
        "#used before, the helpers below offer an option to return results from cache.\n",
        "cache_dir = \"./llm_cache\"\n",
        "def cache_keys_equal(key1,key2):\n",
        "    if (type(key1) is np.ndarray) and (type(key2) is np.ndarray):\n",
        "        return np.array_equal(key1,key2)\n",
        "    return key1==key2\n",
        "\n",
        "def cache_hash(key):\n",
        "    return hashlib.md5(key).hexdigest()\n",
        "\n",
        "def load_cached(key):\n",
        "    cached_name= cache_dir + \"/\" + cache_hash(key)\n",
        "    if os.path.exists(cached_name):\n",
        "        cached=pickle.load(open(cached_name,\"rb\"))\n",
        "        if cache_keys_equal(cached[\"key\"],key):\n",
        "            #cache_copy_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"cache_copy\") #for debugging which files are actually used...\n",
        "            #shutil.copy(cached_name, cache_copy_dir+\"/\" + cache_hash(key))\n",
        "            return cached[\"value\"]\n",
        "    return None\n",
        "\n",
        "def cache(key,value):\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.mkdir(cache_dir)\n",
        "    cached_name= cache_dir + \"/\" + cache_hash(key)\n",
        "    pickle.dump({\"key\":key,\"value\":value},open(cached_name,\"wb\"))\n",
        "\n",
        "\n",
        "#Some info for tokenizing text (e.g., for calculating the number of prompt tokens)\n",
        "tiktoken_encodings = {\n",
        "    \"gpt-4-turbo\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-4-turbo-preview\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-4\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-3.5-turbo\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-3.5-turbo-instruct\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"gpt-3.5-turbo-16k\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"davinci-002\": tiktoken.get_encoding(\"cl100k_base\"),\n",
        "    \"text-davinci-003\": tiktoken.get_encoding(\"p50k_base\"),\n",
        "    \"text-davinci-002\": tiktoken.get_encoding(\"p50k_base\"),\n",
        "    \"text-davinci-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"text-curie-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"text-babbage-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"text-ada-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"davinci\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"curie\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"babbage\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "    \"ada\": tiktoken.get_encoding(\"r50k_base\"),\n",
        "}\n",
        "\n",
        "#Maximum number of tokens supported by different models\n",
        "max_llm_context_length = {\n",
        "    \"gpt-4-turbo\": 16384*2,\n",
        "    \"gpt-4-turbo-preview\": 16384*2,\n",
        "    \"gpt-3.5-turbo-16k\": 16384,\n",
        "    \"gpt-4\": 8192,\n",
        "    \"gpt-3.5-turbo\": 4096,\n",
        "    \"gpt-3.5-turbo-instruct\": 4096,\n",
        "    \"text-davinci-003\": 4096,\n",
        "    \"text-davinci-002\": 4096,\n",
        "    \"text-davinci-001\": 2049,\n",
        "    \"text-curie-001\": 2049,\n",
        "    \"text-babbage-001\": 2049,\n",
        "    \"text-ada-001\": 2049,\n",
        "    \"davinci\": 2049,\n",
        "    \"curie\": 2049,\n",
        "    \"babbage\": 2049,\n",
        "    \"ada\": 2049\n",
        "}\n",
        "\n",
        "#Does a model only support the newer chat API and not the continuations API?\n",
        "def is_chat_model(model):\n",
        "    return (\"gpt-4\" in model) or (\"gpt-3.5-turbo\" in model) and (\"gpt-3.5-turbo-instruct\" not in model)\n",
        "\n",
        "#Calculate the number of tokens for a string\n",
        "def num_tokens_from_string(string: str, model: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    if not model in tiktoken_encodings:\n",
        "        raise Exception(f\"Tiktoken encoding unknown for LLM: {model}\")\n",
        "    encoding = tiktoken_encodings[model]\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Queries an LLM for continuations of a batch of prompts given as a list\n",
        "def query_LLM_batch(model, prompt_batch, max_tokens, use_cache=None, temperature=None,system_message=None,stop=None):\n",
        "    if use_cache is None:\n",
        "        use_cache=False\n",
        "    cache_key=model.join(prompt_batch).encode('utf-8')\n",
        "    if use_cache:\n",
        "        cached_result=load_cached(cache_key)\n",
        "        if cached_result is not None:\n",
        "            return cached_result\n",
        "\n",
        "    #choose whether to use the chat API or the older query API\n",
        "    if is_chat_model(model):\n",
        "        if system_message is None:\n",
        "            system_message = \"You are a helpful assistant.\"\n",
        "\n",
        "        # each batch in the prompt becomes its own asynchronous chat completion request\n",
        "        async def batch_request(prompt_batch):\n",
        "            tasks=[]\n",
        "            for prompt in prompt_batch:\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt},\n",
        "                ]\n",
        "                tasks.append(client_async.chat.completions.create(\n",
        "                    model=model,\n",
        "                    messages=messages,\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                    n=1,  # one completion per prompt\n",
        "                    stop=stop,\n",
        "                    frequency_penalty=0.0,\n",
        "                    presence_penalty=0.0,\n",
        "                ))\n",
        "            return await asyncio.gather(*tasks)\n",
        "\n",
        "        loop = asyncio.get_event_loop()\n",
        "        responses = loop.run_until_complete(batch_request(prompt_batch))\n",
        "        continuations = [response.choices[0].message.content.strip() for response in responses]\n",
        "\n",
        "        # before we return the continuations, ensure that we don't violate OpenAI's rate limits\n",
        "        total_tokens = 0\n",
        "        for prompt in prompt_batch:\n",
        "            total_tokens += num_tokens_from_string(string=system_message, model=model)\n",
        "            total_tokens += num_tokens_from_string(string=prompt, model=model)\n",
        "        for continuation in continuations:\n",
        "            total_tokens += num_tokens_from_string(string=continuation, model=model)\n",
        "        max_tokens_per_minute = 90000  # currently imposed limit for ChatGPT models\n",
        "        wait_seconds = (total_tokens / max_tokens_per_minute) * 60.0\n",
        "        #print(f\"Waiting {wait_seconds} seconds to ensure staying within rate limit\")\n",
        "        time.sleep(wait_seconds)\n",
        "\n",
        "    else:\n",
        "        # The old completions API supports batched prompts out-of-the-box\n",
        "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        response = client.completions.create(\n",
        "            model=model,\n",
        "            prompt=prompt_batch,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=1.0,\n",
        "            frequency_penalty=0.0,\n",
        "            presence_penalty=0.0,\n",
        "            stop=stop,\n",
        "            n=1  # one completion per prompt\n",
        "        )\n",
        "        # extract continuations\n",
        "        continuations = [choice.text for choice in response.choices]\n",
        "\n",
        "        # before we return the continuations, ensure that we don't violate OpenAI's rate limits\n",
        "        total_tokens = 0\n",
        "        for prompt in prompt_batch:\n",
        "            total_tokens += num_tokens_from_string(string=prompt, model=model)\n",
        "        for continuation in continuations:\n",
        "            total_tokens += num_tokens_from_string(string=continuation, model=model)\n",
        "        max_tokens_per_minute = 90000  # currently imposed limit for ChatGPT models\n",
        "        wait_seconds = (total_tokens / max_tokens_per_minute) * 60.0\n",
        "        #print(f\"Waiting {wait_seconds} seconds to ensure staying within rate limit\")\n",
        "        time.sleep(wait_seconds)\n",
        "\n",
        "    if use_cache:\n",
        "        cache(key=cache_key,value=continuations)\n",
        "    return continuations\n",
        "\n",
        "\n",
        "#Query LLM with a list of prompts\n",
        "def query_LLM(model, prompts, max_tokens, use_cache=None, temperature=None, system_message=None,stop=None):\n",
        "    #Query the LLM in batches\n",
        "    continuations=[]\n",
        "    batch_size = 10  # The exact max batch_size for each model is unknown. This seems to work for all, and provides a nice speed-up.\n",
        "    N = len(prompts)\n",
        "    for i in range(0, N, batch_size):\n",
        "        prompt_batch=prompts[i:min([N, i + batch_size])]\n",
        "        continuations+=query_LLM_batch(model=model,\n",
        "                                 prompt_batch=prompt_batch,\n",
        "                                 max_tokens=max_tokens,\n",
        "                                 use_cache=use_cache,\n",
        "                                 temperature=temperature,\n",
        "                                 system_message=system_message,\n",
        "                                 stop=stop)\n",
        "        print_progress_bar(min([N, i + batch_size]), N,printEnd=\"\")\n",
        "    return continuations\n",
        "\n",
        "\n",
        "\n",
        "def embed(texts,use_cache=None,model=None):\n",
        "    if model is None:\n",
        "        model = \"text-embedding-3-small\"\n",
        "    if use_cache is None:\n",
        "        use_cache = True\n",
        "    cache_key=(model+(\"\".join(texts))).encode('utf-8')\n",
        "    if use_cache:\n",
        "        cached_result=load_cached(cache_key)\n",
        "        if cached_result is not None:\n",
        "            print(\"Loaded embeddings from cache, hash\", cache_hash(cache_key))\n",
        "            return cached_result\n",
        "\n",
        "\n",
        "    #query embeddings from the API\n",
        "    texts=[json.dumps(s) for s in texts]  #make sure we escape quotes in a way compatible with GPT-3 API's internal use of json\n",
        "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "    batch_size = 32\n",
        "    N = len(texts)\n",
        "\n",
        "    embed_matrix=[]\n",
        "    for i in range(0, N, batch_size):\n",
        "        print_progress_bar(i, N)\n",
        "        embed_batch=texts[i:min([N, i + batch_size])]\n",
        "        embeddings = client.embeddings.create(input=embed_batch, model=model)\n",
        "        print(embeddings)\n",
        "        for j in range(len(embed_batch)):\n",
        "            embed_matrix.append(embeddings.data[j].embedding)\n",
        "    print(\"\")\n",
        "    embed_matrix=np.array(embed_matrix)\n",
        "    #dim = len(embeddings['data'][0]['embedding'])\n",
        "    #embed_matrix = np.zeros([N, dim])\n",
        "    #for i in range(N):\n",
        "    #    embed_matrix[i, :] = embeddings['data'][i]['embedding']\n",
        "\n",
        "    #update cache\n",
        "    if use_cache:\n",
        "        cache(cache_key,embed_matrix)\n",
        "\n",
        "    #return results\n",
        "    return embed_matrix\n",
        "\n",
        "\n",
        "def reduce_embedding_dimensionality(embeddings,num_dimensions,method=\"UMAP\",use_cache=True,n_neighbors=None):\n",
        "    if isinstance(embeddings,list):\n",
        "        #embeddings is a list of embedding matrices => pack all to one big matrix for joint dimensionality reduction\n",
        "        all_emb = np.concatenate(embeddings, axis=0)\n",
        "    else:\n",
        "        all_emb = embeddings\n",
        "    def unpack(x,embeddings_list):\n",
        "        row = 0\n",
        "        result = []\n",
        "        for e in embeddings_list:\n",
        "            N = e.shape[0]\n",
        "            result.append(x[row:row + N])\n",
        "            row += N\n",
        "        return result\n",
        "\n",
        "    cache_key=(str(all_emb.tobytes())+str(num_dimensions)+method+str(n_neighbors)).encode('utf-8')\n",
        "    if use_cache:\n",
        "        cached_result=load_cached(cache_key)\n",
        "        if cached_result is not None:\n",
        "            print(\"Loaded dimensionality reduction results from cache, hash \", cache_hash(cache_key))\n",
        "            if isinstance(embeddings, list):\n",
        "                return unpack(cached_result,embeddings)\n",
        "            else:\n",
        "                return cached_result\n",
        "    from sklearn.manifold import MDS\n",
        "    from sklearn.manifold import TSNE\n",
        "    import umap\n",
        "    from sklearn.decomposition import PCA\n",
        "    #cosine distance\n",
        "    all_emb=all_emb/np.linalg.norm(all_emb,axis=1,keepdims=True)\n",
        "\n",
        "    if method==\"MDS\":\n",
        "        mds=MDS(n_components=num_dimensions,dissimilarity=\"precomputed\")\n",
        "        cosine_sim = np.inner(all_emb, all_emb)\n",
        "        cosine_dist = 1 - cosine_sim\n",
        "        x=mds.fit_transform(cosine_dist)\n",
        "    elif method==\"TSNE\":\n",
        "        tsne=TSNE(n_components=num_dimensions)\n",
        "        x=tsne.fit_transform(all_emb)\n",
        "    elif method==\"PCA\":\n",
        "        pca=PCA(n_components=num_dimensions)\n",
        "        x=pca.fit_transform(all_emb)\n",
        "    elif method==\"UMAP\":\n",
        "        if n_neighbors is None:\n",
        "            n_neighbors=5\n",
        "        reducer = umap.UMAP(n_components=num_dimensions,metric='cosine',n_neighbors=n_neighbors)\n",
        "        x=reducer.fit_transform(all_emb)\n",
        "    else:\n",
        "        raise Exception(\"Invalid dimensionality reduction method!\")\n",
        "\n",
        "    if use_cache:\n",
        "        cache(cache_key,x)\n",
        "\n",
        "    if isinstance(embeddings, list):\n",
        "        return unpack(x,embeddings)\n",
        "    return x\n",
        "\n",
        "\n",
        "#some quick test methods\n",
        "def test_embeddings():\n",
        "    texts=[\"queen\",\"king\",\"man\",\"woman\"]\n",
        "    embeddings=embed(texts)\n",
        "    embeddings=reduce_embedding_dimensionality(embeddings,method=\"PCA\",num_dimensions=2)\n",
        "    df=pd.DataFrame()\n",
        "    df[\"texts\"]=texts\n",
        "    df[\"x\"]=embeddings[:,0]\n",
        "    df[\"y\"]=embeddings[:,1]\n",
        "    px.scatter(df,\n",
        "                #width=1300, height=1000, #The codes should be approximately 1:1 aspect ratio, but need space for the color bar\n",
        "                x=\"x\",\n",
        "                y=\"y\",\n",
        "                hover_name=\"texts\")\n",
        "\n",
        "def test_batched_prompting():\n",
        "    prompts=[\"what is 1+1?\",\"what is 1+2?\",\"what is 1+3?\"]\n",
        "    print(query_LLM(\"davinci-002\",prompts,max_tokens=20))\n"
      ],
      "metadata": {
        "id": "AJohDVequMZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08iFJ77eYv3w"
      },
      "source": [
        "### Basic few-shot prompting of game ideas\n",
        "The code below simply does the same basic few-shot prompting as we did above in the OpenAI Playground\n",
        "\n",
        "**Exercise:**\n",
        "1. Run the code by clicking on the triangle. Repeat this a few times to see how the results are different every time. This is generally how one works in Colab, running the code cell by cell.\n",
        "2. Change the model to gpt-3.5-turbo-instruct using the drop-down menu and run the code again. This demonstrates how Colab provides tools for building simple UIs for one's code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEOaOMUcYZTM"
      },
      "outputs": [],
      "source": [
        "model=\"davinci-002\" # @param [\"davinci-002\", \"gpt-3.5-turbo-instruct\"] {allow-input: false}\n",
        "\n",
        "#Define the prompt\n",
        "prompt=\"\"\"A list of novel and experimental indie game ideas:\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#Query the OpenAI completions API\n",
        "response = client.completions.create(\n",
        "  model=model,\n",
        "  prompt=prompt,\n",
        "  temperature=1,\n",
        "  max_tokens=500,\n",
        ")\n",
        "\n",
        "#Print the response\n",
        "print(response.choices[0].text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYSD_F5watPt"
      },
      "source": [
        "### Few-shot prompting of long lists\n",
        "Sometimes, one wants to generate a list of multiple items in one go.\n",
        "\n",
        "However, LLMs always add the generated tokens to the \"prompt\" they use for the next tokens. Thus, *previously generated items become new few-shot examples.* This means that even one bad generation can throw the LLM off the rails and quality generally decreases with each item due to small random errors accumulating.\n",
        "\n",
        "To prevent the error accumulation, it's better to prompt each item separately, making sure that the LLM only sees the original examples.\n",
        "\n",
        "**Caution:** This code allows generating up to 100 items at a time. This can start eating your API quota quite fast, so the n_generated parameter is set to 10 by default.\n",
        "\n",
        "**Exercises:**\n",
        "1. Run the code by clicking on the triangle.\n",
        "2. Edit the prompt to try improving the generation quality, similar to what you practiced above in Part 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBqqxu0JVe0f"
      },
      "outputs": [],
      "source": [
        "#Define the prompt (you can freely edit this)\n",
        "prompt=\"\"\"A list of novel and experimental indie game ideas:\n",
        "\n",
        "---\n",
        "\n",
        "An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \"floor\", \"is\", and \"lava\" to make the floor deadly.\n",
        "\n",
        "---\n",
        "\n",
        "An FPS puzzle game with a \"portal gun\" that can create portals between two flat planes. For example, the player can create one portal on the floor and the second on the ceiling and push an object so that it falls down the floor portal and drops from the ceiling portal on top of some target.\n",
        "\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#parameters for the user\n",
        "model=\"davinci-002\" # @param [\"davinci-002\", \"gpt-3.5-turbo-instruct\"] {allow-input: false}\n",
        "temperature=0.7 #@param {type:\"slider\", min:0, max:1,step:0.1}\n",
        "n_generated=10 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "max_tokens=400 #@param {type:\"slider\", min:50, max:500, step:10}\n",
        "\n",
        "#define what separates each list item in the prompt\n",
        "list_delimiter=\"---\"\n",
        "\n",
        "#Helper function: Generate multiple continuations, returned as a list of strings\n",
        "def generate_multiple(model,prompt,max_tokens,n):\n",
        "  response = client.completions.create(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=1,\n",
        "    max_tokens=max_tokens,\n",
        "    stop=list_delimiter,  #stop after we encounter the list delimiter => only generate one idea per prompt\n",
        "    n=n #here, we generate multiple alternative continuations\n",
        "  )\n",
        "  return [choice.text.strip() for choice in response.choices] #strip trailing/leading spaces\n",
        "\n",
        "#use the prompt to generate multiple list items\n",
        "items=generate_multiple(model=model,\n",
        "                        prompt=prompt,\n",
        "                        max_tokens=max_tokens,\n",
        "                        n=n_generated)\n",
        "\n",
        "#show the results as a table\n",
        "df=pd.DataFrame()\n",
        "df[\"items\"]=items\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Automatically prune the generated ideas.\n",
        "Generating lists like above can be needed in hard tasks where many of the results are bad. However, identifying the rare good generations can be tedious.\n",
        "\n",
        "Here, we test if AI can help in discovering the best results. For this task (analytical and convergent thinking instead of a divergent and creative thinking), it's best to use the most advanced LLMs such as GPT-4.\n",
        "\n",
        "To get you started, the code below simply asks GPT-4 to say which of the ideas generated above is the best one.\n",
        "\n",
        "**Exercise:**\n",
        "- Run the code. Do you agree with GPT-4's choice?\n",
        "\n",
        "\n",
        "**Challenge for programmers:** Improve the code. For example, you could:\n",
        "\n",
        "- Elaborate on the prompt: How should the ideas be evaluated? What makes an idea good? For example, you might consider novelty and whether the mechanics or design hook are described with enough detail for someone to implement a prototype.\n",
        "- Ask for numerical scores instead, parse them from the model output, and sort the table\n",
        "- Add few-shot examples of ideas and how they should be scored\n"
      ],
      "metadata": {
        "id": "ReL3g_6Xvab3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"Here is a list of indie game ideas:\\n\\n\"\n",
        "for count,item in enumerate(items):\n",
        "  prompt+=f\"Idea {count}: \"+item + \"\\n\\n\"\n",
        "\n",
        "prompt+=\"Which of the ideas is the best one? Please first explain your rationale and then state which idea is the best one and why. To conclude, reprint the selected idea verbatim.\"\n",
        "#print(\"Querying GPT-4 with the prompt:\\n\\n\\n\")\n",
        "#print(prompt)\n",
        "responses=query_LLM(model=\"gpt-4-turbo\",\n",
        "                prompts=[prompt], #query_LLM helper expects a list of prompts (optimized for batched prompting)\n",
        "                max_tokens=400)\n",
        "print(responses[0])\n"
      ],
      "metadata": {
        "id": "Oklc4lOAwYqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For further analysis, generate many ideas with both the base model and finetuned model.\n",
        "\n",
        "To minimize API cost, the default is that we load pre-generated ideas. Uncheck the \"load_generated\" checkbox to generate new ideas."
      ],
      "metadata": {
        "id": "h8XHu6EPoSiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_pregenerated=True #@param {type: \"boolean\"}\n",
        "ideas_per_model=50 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "print(f\"Generating {2*ideas_per_model} ideas using the following prompt:\\n\\n\")\n",
        "print(prompt)\n",
        "if load_pregenerated:\n",
        "  df=pd.read_csv(\"https://drive.google.com/uc?export=download&id=19QCJfmJNiExRNlEYEHzGXFlgytAAszim\")\n",
        "else:\n",
        "  items=generate_multiple(model=\"davinci-002\",\n",
        "                        prompt=prompt,\n",
        "                        max_tokens=max_tokens,\n",
        "                        n=ideas_per_model)\n",
        "  items+=generate_multiple(model=\"gpt-3.5-turbo-instruct\",\n",
        "                        prompt=prompt,\n",
        "                        max_tokens=max_tokens,\n",
        "                        n=ideas_per_model)\n",
        "\n",
        "  #convert the items to a Pandas DataFrame (basically, an Excel sheet)\n",
        "  df=pd.DataFrame()\n",
        "  df[\"items\"]=items\n",
        "  df[\"model\"]=\"davinci-002\"\n",
        "  df[\"model\"][ideas_per_model:]=\"gpt-3.5-turbo-instruct\"\n",
        "  df.to_csv(\"generated_items.csv\")\n",
        "\n",
        "#Make a copy of the results and show it as a table\n",
        "df_all=df.copy()\n",
        "df_all"
      ],
      "metadata": {
        "id": "XWVLRo4joRYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwCDaUDTstwu"
      },
      "source": [
        "### Visualizing generations using embeddings\n",
        "Here, we demonstrate how embedding vectors can be used for text data visualization and exploration.\n",
        "\n",
        "**What are embedding vectors?**\n",
        "\n",
        "Modern language models can be used to map any piece of text into real-valued vectors (arrays of floating point values) that encode the meaning of the text in an abstract \"embedding space\". Similar texts will produce similar embedding vectors (typically, in terms of cosine similarity).\n",
        "\n",
        "**Exercise:**\n",
        "\n",
        "Run the code and browse the generated scatter plot using a mouse. What kind of clusters you notice? Are the ideas on the edges of the graph more weird or interesting than the ideas in the center? Are there any differences between the davinci-002 and gpt-3.5-turbo-instruct?\n",
        "\n",
        "You can try different diminsionality reduction methods to see different clusters form. UMAP often considered as the most advanced method forms more clear clusters, but calculating it can take a long time."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dimensionality_reduction=\"MDS\" # @param [\"PCA\", \"MDS\", \"UMAP\"] {allow-input: false}\n",
        "df=df_all.copy() #use a copy of the dataframe we generated or loaded above\n",
        "items=df[\"items\"].to_list()\n",
        "\n",
        "#calculate embeddings using our OpenAI API helper\n",
        "embeddings=embed(items)\n",
        "\n",
        "#Reduce the dimensionality to 2d for visualization\n",
        "embeddings=reduce_embedding_dimensionality(embeddings,\n",
        "                                           method=dimensionality_reduction,\n",
        "                                           num_dimensions=2)\n",
        "\n",
        "#Add plotting data to the df\n",
        "hover_texts=[\"</br>\".join(textwrap.wrap(item,width=60)) for item in items]\n",
        "df[\"Hover\"]=hover_texts\n",
        "df[\"x\"] = embeddings[:,0]\n",
        "df[\"y\"] = embeddings[:,1]\n",
        "\n",
        "# Plot using Plotly's scatter plot functionality\n",
        "px.scatter(df,\n",
        "             width=800, height=800,\n",
        "             x=\"x\",\n",
        "             y=\"y\",\n",
        "             hover_name=\"Hover\",\n",
        "             color = \"model\")\n"
      ],
      "metadata": {
        "id": "XTB7X8ICKRQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5Ef6F48YRYW"
      },
      "source": [
        "### Combinatorial few-shot prompting\n",
        "A common creativity method for humans is to pick random elements and recombine them. For example, the VNA game ideation method combines a random Verb, Noun, and Adjective.\n",
        "\n",
        "This can help one think outside the box, and it can also be useful to force an LLM to generate more diverse ideas, especially when using a finetuned model that by default produces high quality but less diverse generations.\n",
        "\n",
        "Here, we extend few-shot prompting with randomly combined modifiers: We prompt game ideas by *systematically combining genres and mechanics*.\n",
        "\n",
        "In this example, we also show how to provide a more comprehensive UI and generating the prompt programmatically using data input via the UI.\n",
        "\n",
        "**Exercises:**\n",
        "* Run the code a few times and try different models. Try adding a third modifier, e.g., \"Single Player, Multiplayer\".\n",
        "\n",
        "* Change the prompt start, modifiers, and examples to generate different kind of games. For example, you could try combining classic games such as Space Invaders and Pac Man with different emotions such as sadness, love, grief, break-up. For instance, a Space Invaders + Break-up could have the enemies as photos of the player and their ex, and when the player shoots them, they get deleted from their photos folder. The shields could be frieds of the player and their ex who become collateral damage in a break-up. To get some seed ideas, you can try to continue prompts like \"My experimental game combines Space Invaders with the emotion of Love. The player\" in the OpenAI Playground.\n",
        "\n",
        "* Modify the prompt start, examples, and modifiers to prompt spell names for a game where each spell combines different elements such as earth, wind, fire, and blood.\n",
        "\n",
        "**Important:** Separate the modifiers using commas. When changing the examples, remember to start each example with a modifier combination."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxkBhJk5z9N7"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "model=\"davinci-002\" # @param [\"davinci-002\", \"gpt-3.5-turbo-instruct\"] {allow-input: false}\n",
        "prompt_start=\"A list of novel experimental indie game ideas:\" # @param {type:\"string\"}\n",
        "modifiers_1=\"FPS,Puzzle,Dating sim,Platformer\" # @param {type:\"string\"}\n",
        "modifiers_2=\"Time manipulation,Rule manipulation,Camera manipulation\" # @param {type:\"string\"}\n",
        "modifiers_3=\"\" # @param {type:\"string\"}\n",
        "modifiers_4=\"\" # @param {type:\"string\"}\n",
        "example1=\"FPS + Time manipulation: An FPS game where time only moves when the player moves or performs actions. This allows Matrix-style slow-motion gun ballet, and transforms real-time action into a puzzle.\" # @param {type:\"string\"}\n",
        "example2=\"Puzzle + Rule manipulation: A Sokoban-style game where the player pushes around blocks that are variables, operators, and definitions of a programming language. Blocks that connect to each other form statements that allow the player to define and alter the game's rules. For example, the player can connect \\\"floor\\\", \\\"is\\\", and \\\"lava\\\" to make the floor deadly.\" # @param {type:\"string\"}\n",
        "example3=\"Platformer + Camera manipulation: An FPS puzzle game with a \\\"portal gun\\\" that can create portals between two flat planes. For example, the player can create one portal on the floor and the second on the ceiling and push an object so that it falls down the floor portal and drops from the ceiling portal on top of some target.\" # @param {type:\"string\"}\n",
        "n_generated=4 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "#define what separates each list item in the prompt\n",
        "list_delimiter=\"---\"\n",
        "\n",
        "#Helper function: Construct the prompt with examples\n",
        "def construct_prompt(prompt_start,examples):\n",
        "  prompt=prompt_start\n",
        "  for example in examples:\n",
        "    if len(example)>0 and (not str.isspace(example)):\n",
        "      prompt+=\"\\n\\n\"+list_delimiter+\"\\n\\n\"+example\n",
        "  prompt+=\"\\n\\n\"+list_delimiter+\"\\n\\n\"\n",
        "  return prompt\n",
        "\n",
        "#Construct a list of all possible combinations of the modifiers\n",
        "modifiers=[modifiers_1,modifiers_2,modifiers_3,modifiers_4]\n",
        "combined_items=[]\n",
        "for modifier in modifiers:\n",
        "  if len(modifier)>0:\n",
        "    combined_items.append(modifier.split(\",\"))\n",
        "combinations = list(itertools.product(*combined_items))\n",
        "\n",
        "#If there's more combinations than the user requests, take a random subset\n",
        "if n_generated < len(combinations):\n",
        "  np.random.shuffle(combinations)\n",
        "  combinations=combinations[:n_generated]\n",
        "\n",
        "#Format the combinations as strings that can be added to the prompt\n",
        "combination_strings=[\" + \".join(combination)+\":\" for combination in combinations]\n",
        "\n",
        "#Construct the prompts\n",
        "prompt_base=construct_prompt(prompt_start,examples)\n",
        "prompts=[prompt_base+combination_string for combination_string in combination_strings]\n",
        "\n",
        "#Generate items. Here, instead of using the OpenAI API directly,\n",
        "#we use our own helper for batches of multiple prompts\n",
        "items=query_LLM(model=model,\n",
        "                prompts=prompts,\n",
        "                max_tokens=max_tokens,\n",
        "                stop=list_delimiter)\n",
        "\n",
        "#Add the combination strings back to the generated items for readability\n",
        "for i,comb in enumerate(combination_strings):\n",
        "  items[i]=comb+items[i]\n",
        "\n",
        "#Print results\n",
        "for item in items:\n",
        "  print(item+\"\\n\\n\")\n",
        "\n",
        "#Also show the results as a data table, which may be more readable\n",
        "df=pd.DataFrame()\n",
        "df[\"items\"]=items\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Yilz7X-WNYeQ",
        "_qFP8548OUjJ",
        "OvXpXUB5fEwt"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}